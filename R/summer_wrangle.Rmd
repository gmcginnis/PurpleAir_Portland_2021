---
title: "summer_wrangle"
author: "Gillian McGinnis"
date: "6/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(data.table)
library(lubridate)
```

```{r manual_inputs}
# Path to folder containing CSV files of interest
input_path <- "data/raw"

# Choose to load primary and/or secondary data. At least one must be TRUE
input_read_primary <- TRUE
input_read_secondary <- TRUE

#OPTIONAL: filter for specific locations, e.g. for STAR labs use "*STAR*" exactly. Use NULL otherwise
input_path_filter <- "PSU STAR LAB SEL"
#input_path_filter <- "se"

input_timezone <- "America/Los_Angeles"
```

```{r}
# Optional grouping settings
## DATE GROUPING: group values into tags (will NOT impact raw data output, but can be useful when graphing)
# Run date grouping? If set to "FALSE", the date inputs below will not matter (but will still appear in the environment)
run_date_grouping <- TRUE
# Date grouping categories:
input_date_tags <- c("Before", "Fire", "After")
# Start dates (in a list, "YYYY-MM-DD")
input_date_starts <- c("2020-09-01", "2020-09-10", "2020-09-20")
# End dates (in a list, "YYYY-MM-DD")
input_date_ends <- c("2020-09-09", "2020-09-19", "2020-09-30")

## HOUR GROUPING: group values into tags (will NOT impact raw data output, but can be useful when graphing)
# Run hour grouping? If set to "FALSE", the hour inputs below will not matter (but will still appear in the environment)
run_hour_grouping <- TRUE
# Hour grouping categories:
input_hour_tags <- c("Morning", "Afternoon", "Evening", "Night")
# Start hours (in a list, using full hours in 24 hour format)
input_hour_starts <- c(5, 12, 17, 21)
# End hours will be automatically generated
```


```{r settings}
# list of (ORIGINAL name) variables to skip when reading file
input_skips <- c(
  "entry_id",
  "UptimeMinutes",
  "V11" #Auto added by fread b/c of how the CSVs are stored
)

# Use format of "Original name" = "new_name". Caveat: do NOT rename the timestamp variable here
input_renames <- c(
  "PM1.0_CF1_ug/m3" = "pm1_cf1",
  "PM2.5_CF1_ug/m3" = "pm25_cf1",
  "PM10.0_CF1_ug/m3" = "pm10_cf1",
  "UptimeMinutes" = "uptime",
  "RSSI_dbm" = "rssi",
  "Temperature_F" = "temperature",
  "Humidity_%" = "humidity",
  "PM1.0_ATM_ug/m" = "pm1_atm",
  "PM2.5_ATM_ug/m3" = "pm25_atm",
  "PM10_ATM_ug/m3" = "pm10_atm"
)

# Use format of "created_at" = "new_name". Rename timestamp variable
#input_rename_timestamp <- c("created_at" = "datetime")

# List of renamed variables to keep (timestamp automatically kept)
input_selects <- c(
  "pm25_cf1",
  "pm25_atm",
  "temperature",
  "humidity"
  # "pm1_cf1",
  # "pm1_atm",
  # "pm10_cf1",
  # "pm10_atm"
)


file_list <- list.files(path = input_path,
                        pattern = input_path_filter,
                        full.names = TRUE)
```

```{r location_index}
# Input: list of files in path
# Output: data frame with sensor tag, latitude, longitude, inside/outside, timezone
data_pa_meta <- data.frame(source = file_list) %>% 
  mutate(
    loc_path = str_extract(source, "\\w.+(?= \\((outside|inside|undefined))"),
    label = str_replace(loc_path, paste(input_path, "/", sep = ""), ""),
    label = str_replace(label, " B$", ""),
    loc_inout = str_extract(source, "(?!\\()(outside|inside)(?=\\))"),
    lat = str_extract(source, "(?!\\()-*\\d+\\.\\d+(?= )"),
    lon = str_extract(source, "-*\\d+\\.\\d+(?=\\))")
  ) %>% 
  select(!c(source, loc_path)) %>% 
  group_by(label) %>%
  fill(loc_inout) %>%
  unique() %>% 
  mutate(
    loc_inout = factor(loc_inout, levels = c("outside", "inside")),
    lat = as.numeric(lat),
    lon = as.numeric(lon)
  ) %>% 
  # Renaming to keep consistency with API version
  rename(
    "longitude" = lon,
    "latitude" = lat,
    "DEVICE_LOCATIONTYPE" = loc_inout
  ) %>% 
  mutate(
    site_id = label,
    timezone = input_timezone
  )
```

```{r cleaning_function}
# Function purpose: read specific file by location, skipping columns as needed; extracting info from title
# Input: One file path
# Output: Data frame of input file with relevant info in columns
clean_data <- function(input_file, renames = input_renames, selects = input_selects, skips = input_skips){
  fread(input_file,
        sep = ",",
        fill = TRUE) %>% 
    select(!intersect(all_of(skips), names(.))) %>% 
    plyr::rename(warn_missing = FALSE, renames) %>% 
    select(created_at, intersect(all_of(selects), names(.))) %>%
    mutate(datetime = as.POSIXct(created_at, tz = "UTC")) %>% 
    select(!created_at) %>% 
    mutate(
      source = input_file,
      loc_path = str_extract(source, "\\w.+(?= \\((outside|inside|undefined))"),
      label = str_replace(loc_path, paste(input_path, "/", sep = ""), ""),
      label = str_replace(label, " B$", ""),
      loc_ab = str_extract(source, "B(?= \\((outside|inside|undefined))"),
      loc_ab = replace_na(loc_ab, "A"),
      loc_ab = factor(loc_ab, levels = c("A", "B"))
    ) %>% 
    select(!c(source, loc_path)) %>% 
    # Adding site_id variable so that same functions from API can also apply here
    rename(site_id = label)
}
```

```{r iteration}
data_primary <- data.frame()
data_secondary <- data.frame()
count_active <- 0
count_total <- length(file_list)

# Loop purpose: repeat file reading process for all files in a specified path
# Output: data frames (one primary, one secondary)
for(file_single in file_list){
  
  if(input_read_primary == FALSE & input_read_secondary == FALSE){
    stop("INPUT ERROR: Execution halted. Please set input read options to TRUE for primary and/or secondary.")
  }
  
  count_active <- count_active + 1
  print(sprintf("Working on %d/%d (%s) ...", count_active, count_total, file_single))
  
  if(
    input_read_primary == TRUE &
    str_detect(file_single, "\\) Primary") == TRUE
    ){
    temp_primary <- clean_data(file_single)
    
    data_primary <- plyr::rbind.fill(data_primary, temp_primary)
    
    remove(temp_primary)
  }
  
  if(
    input_read_secondary == TRUE &
    str_detect(file_single, "\\) Secondary") == TRUE
    ){
    temp_secondary <- clean_data(file_single)
    
    data_secondary <- plyr::rbind.fill(data_secondary, temp_secondary)
    
    remove(temp_secondary)
  }
  
  remove(file_single)
  
}
```

```{r eclean_iteration}
remove(clean_data, count_active, count_total, input_path, input_renames, input_selects, input_skips)
```


```{r combine}
if(input_read_primary == TRUE & input_read_secondary == TRUE){
  raw_pa_data <- data_primary %>% 
    full_join(data_secondary)
  message_combine <- "Primary and secondary data frames now joined."
}
if(input_read_secondary == FALSE){
  raw_pa_data <- data_primary
  message_combine <- "Only primary data selected."
}
if(input_read_primary == FALSE){
  raw_pa_data <- data_secondary
  message_combine <- "Only secondary data selected."
}

print(message_combine)
```

```{r eclean_combine}
remove(message_combine, data_primary, data_secondary)
```


```{r qc}
cols_to_pivot <- raw_pa_data %>%
  # Selecting one row, for ease of processing
  slice(1) %>% 
  # Selecting all numeric columns
  select_if(is.numeric) %>% 
  # B does not report temperature and humidity, therefore we will not pivot them
  select(!intersect(c("temperature", "humidity"), names(.))) %>% 
  names()

data_pa <- raw_pa_data %>% 
  pivot_wider(names_from = loc_ab, values_from = all_of(cols_to_pivot)) %>% 
  mutate(datetime = floor_date(datetime, unit = "2 minutes")) %>% 
  group_by(site_id, datetime) %>% 
  summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
  apply_qc() %>% 
  adjust_timezone(location_data = data_pa_meta) %>% 
  select(site_id, datetime, everything())

remove(cols_to_pivot)
```

```{r dates}
input_startdate <- min((data_pa %>% column_dt("date"))$date)
input_enddate <- max((data_pa %>% column_dt("date"))$date)
```


```{r auto_apply}
data_pa_hourly <- apply_functions(data_pa, by_day = TRUE, by_hour = TRUE)
data_pa_daily <- apply_functions(data_pa, by_day = TRUE, by_hour = FALSE)
data_pa_diurnal <- apply_functions(data_pa, by_day = FALSE, by_hour = TRUE)
```

```{r manual_apply, eval=FALSE}
data_pa_hourly <- data_pa %>% 
  apply_corrections(daily = TRUE, hourly = TRUE) %>% 
  apply_hour_tags() %>% 
  apply_date_tags()

data_pa_daily <- data_pa %>% 
  apply_corrections(daily = TRUE, hourly = FALSE) %>% 
  apply_date_tags()
```

```{r mapping_test, eval=FALSE}
data_pa_hourly %>%
  map_q(value = pm25_atm, location_data = data_pa_meta, grouping_var = hour_tag)

long_set <- data_pa_hourly %>% 
  pivot_longer(cols = c(pm25_cf1, epa_2020, epa_2021, epa_atm, lrapa, pm25_atm),
               names_to = "corr",
               values_to = "pm25") %>% 
  mutate(
    corr = fct_inorder(corr),
    source = fct_collapse(corr,
      PA = c("pm25_cf1", "pm25_atm"),
      EPA = c("epa_2020", "epa_2021", "epa_atm"),
      LRAPA = "lrapa"
    )
  )

long_set %>% 
  drop_na(date_tag) %>% 
  ggplot(aes(date_hour, pm25, color = corr, linetype = source)) +
  facet_grid(cols=vars(date_tag), scales="free") +
  geom_line() +
  theme(legend.position = "bottom")
  

  
long_set %>% 
  drop_na(date_tag) %>% 
  #ggplot(aes(source, value, color = name)) +
  ggplot(aes(corr, pm25, color = source)) +
  #facet_wrap(~date_tag, scales = "free") +
  facet_grid(cols=vars(date_tag), scales="free") +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), linetype = "dotted") +
  geom_violin(draw_quantiles = 0.5, fill = "transparent") +
  stat_summary(fun.y = mean, geom ="point", color = "red") +
  theme(legend.position = "bottom")

long_set %>% 
  filter(str_detect(date_tag, "Before|After")) %>% 
  mutate(wday = lubridate::wday(date, label=TRUE),
         blah=hms(format(date_hour, format="%H:%M:%S"))) %>% 
  select(date_hour, blah, wday, hour, hour_tag, corr, pm25, source) %>% 
  ggplot(aes(x=blah, y=pm25, color=corr)) +
  scale_x_time() +
  facet_grid(cols=vars(wday), rows=vars(source)) +
  geom_line()

data_pa_hourly %>% 
  drop_na(date_tag) %>% 
  #filter(str_detect(date_tag, "Before")) %>% 
  #filter(str_detect(date_tag, "Before|After")) %>% 
  mutate(wday=lubridate::wday(date,label=TRUE)) %>% 
  group_by(site_id,date_tag,hour,wday) %>% 
  summarize_if(is.numeric, mean, na.rm=TRUE) %>% 
  ggplot(aes(x = hour, y = epa_2021,color=date_tag)) +
  #facet_grid(cols = vars(wday)) +
  facet_grid(date_tag~wday, scales="free_y") +
  geom_line() +
  theme(legend.position="bottom")
```


