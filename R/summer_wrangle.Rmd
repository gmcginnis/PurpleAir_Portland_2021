---
title: "summer_wrangle"
author: "Gillian McGinnis"
date: "6/11/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(data.table)
library(lubridate)
```

```{r manual_inputs}
# Path to folder containing CSV files of interest
input_path <- "data/raw"

# Choose to load primary and/or secondary data. At least one must be TRUE
input_read_primary <- TRUE
input_read_secondary <- FALSE

#OPTIONAL: filter for specific locations, e.g. for STAR labs use "*STAR*" exactly. Use NULL otherwise
input_path_filter <- "PSU STAR LAB SEL"
#input_path_filter <- "se"

input_timezone <- "America/Los_Angeles"
```

```{r settings}
# list of (ORIGINAL name) variables to skip when reading file
input_skips <- c(
  "entry_id",
  "UptimeMinutes",
  "V11" #Auto added by fread b/c of how the CSVs are stored
)

# Use format of "Original name" = "new_name". Caveat: do NOT rename the timestamp variable here
input_renames <- c(
  "PM1.0_CF1_ug/m3" = "pm1_cf1",
  "PM2.5_CF1_ug/m3" = "pm25_cf1",
  "PM10.0_CF1_ug/m3" = "pm10_cf1",
  "UptimeMinutes" = "uptime",
  "RSSI_dbm" = "rssi",
  "Temperature_F" = "temperature",
  "Humidity_%" = "humidity",
  "PM1.0_ATM_ug/m" = "pm1_atm",
  "PM2.5_ATM_ug/m3" = "pm25_atm",
  "PM10_ATM_ug/m3" = "pm10_atm"
)

# Use format of "created_at" = "new_name". Rename timestamp variable
#input_rename_timestamp <- c("created_at" = "datetime")

# List of renamed variables to keep (timestamp automatically kept)
input_selects <- c(
  "pm25_cf1",
  "pm25_atm",
  "temperature",
  "humidity"
  # "pm1_cf1",
  # "pm1_atm",
  # "pm10_cf1",
  # "pm10_atm"
)


file_list <- list.files(path = input_path,
                        pattern = input_path_filter,
                        full.names = TRUE)
```

```{r location_index}
# Input: list of files in path
# Output: data frame with sensor tag, latitude, longitude, inside/outside
data_loc <- data.frame(source = file_list) %>% 
  mutate(
    loc_path = str_extract(source, "\\w.+(?= \\((outside|inside|undefined))"),
    label = str_replace(loc_path, paste(input_path, "/", sep = ""), ""),
    label = str_replace(label, " B$", ""),
    loc_inout = str_extract(source, "(?!\\()(outside|inside)(?=\\))"),
    lat = str_extract(source, "(?!\\()-*\\d+\\.\\d+(?= )"),
    lon = str_extract(source, "-*\\d+\\.\\d+(?=\\))")
  ) %>% 
  select(!c(source, loc_path)) %>% 
  group_by(label) %>%
  fill(loc_inout) %>%
  unique() %>% 
  mutate(
    loc_inout = factor(loc_inout, levels = c("outside", "inside")),
    lat = as.numeric(lat),
    lon = as.numeric(lon)
  ) %>% 
  # Renaming to keep consistency with API version
  rename(
    "longitude" = lon,
    "latitude" = lat,
    "DEVICE_LOCATIONTYPE" = loc_inout
  ) %>% 
  mutate(site_id = label)
```

```{r cleaning_function}
# Function purpose: read specific file by location, skipping columns as needed; extracting info from title
# Input: One file path
# Output: Data frame of input file with relevant info in columns
clean_data <- function(input_file, renames = input_renames, selects = input_selects, skips = input_skips, time_zone = input_timezone){
  fread(input_file,
        sep = ",",
        #drop = input_skips,
        fill = TRUE) %>% 
    select(!intersect(all_of(skips), names(.))) %>% 
    plyr::rename(warn_missing = FALSE, renames) %>% 
    select(created_at, intersect(all_of(selects), names(.))) %>%
    mutate(
      datetime_utc = as.POSIXct(created_at, tzone = "UTC"),
      datetime_tz = lubridate::with_tz(datetime_utc, tzone = time_zone),
      # 2 minute intervals are used when averaging A&B sensor data if unaveraged data is used
      datetime = floor_date(datetime_tz, unit = "2 minutes")
      #datetime = lubridate::with_tz(datetime_utc, "Etc/GMT+8")
    ) %>% 
    select(!created_at) %>% 
    mutate(
      source = input_file,
      loc_path = str_extract(source, "\\w.+(?= \\((outside|inside|undefined))"),
      label = str_replace(loc_path, paste(input_path, "/", sep = ""), ""),
      label = str_replace(label, " B$", ""),
      loc_ab = str_extract(source, "B(?= \\((outside|inside|undefined))"),
      loc_ab = replace_na(loc_ab, "A"),
      loc_ab = factor(loc_ab, levels = c("A", "B"))
    ) %>% 
    select(!c(source, loc_path)) %>% 
    # Adding site_id variable so that same functions from API can also apply here
    mutate(site_id = label)
}
```

```{r iteration}
data_primary <- data.frame()
data_secondary <- data.frame()
count_active <- 0
count_total <- length(file_list)

# Loop purpose: repeat file reading process for all files in a specified path
# Output: data frames (one primary, one secondary)
for(file_single in file_list){
  
  if(input_read_primary == FALSE & input_read_secondary == FALSE){
    stop("INPUT ERROR: Execution halted. Please set input read options to TRUE for primary and/or secondary.")
  }
  
  count_active <- count_active + 1
  print(sprintf("Working on %d/%d (%s) ...", count_active, count_total, file_single))
  
  if(
    input_read_primary == TRUE &
    str_detect(file_single, "\\) Primary") == TRUE
    ){
    temp_primary <- clean_data(file_single)
    
    data_primary <- plyr::rbind.fill(data_primary, temp_primary)
    
    remove(temp_primary)
  }
  
  if(
    input_read_secondary == TRUE &
    str_detect(file_single, "\\) Secondary") == TRUE
    ){
    temp_secondary <- clean_data(file_single)
    
    data_secondary <- plyr::rbind.fill(data_secondary, temp_secondary)
    
    remove(temp_secondary)
  }
  
  remove(file_single)
  
}

#remove(input_read_primary, input_read_secondary, count_active, count_total)
remove(clean_data, count_active, count_total, input_path, input_renames, input_selects, input_skips)
```

```{r combine}
if(input_read_primary == TRUE & input_read_secondary == TRUE){
  data_pa_raw <- data_primary %>% 
    full_join(data_secondary)
  message_combine <- "Primary and secondary data frames now joined."
}
if(input_read_secondary == FALSE){
  data_pa_raw <- data_primary
  message_combine <- "Only primary data selected."
}
if(input_read_primary == FALSE){
  data_pa_raw <- data_secondary
  message_combine <- "Only secondary data selected."
}

print(message_combine)
remove(message_combine, data_primary, data_secondary)
```

```{r qc}
cols_to_pivot <- data_pa_raw %>%
  # Selecting one row, for ease of processing
  slice(1) %>% 
  # Selecting all numeric columns
  select_if(is.numeric) %>% 
  # B does not report temperature and humidity, therefore we will not pivot them
  select(!intersect(c("temperature", "humidity"), names(.))) %>% 
  names()

data_pa <- data_pa_raw %>% 
  group_by(site_id, datetime) %>% 
  pivot_wider(names_from = loc_ab, values_from = all_of(cols_to_pivot)) %>% 
  group_by(site_id, datetime) %>% 
  summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
  apply_qc()
```

```{r corrections}
data_pa_hourly <- data_pa %>% 
  apply_corrections(daily = TRUE, hourly = TRUE) %>% 
  apply_hour_tags() %>% 
  apply_date_tags()

data_pa_daily <- data_pa %>% 
  apply_corrections(daily = TRUE, hourly = FALSE) %>% 
  apply_date_tags()
```

```{r mapping_test, eval=FALSE}
data_pa_hourly %>%
  rename(site_id = label) %>%
  map_q(value = pm25_atm, location_data = (data_loc %>% rename(site_id = label)), grouping_var = hour_tag)
```


