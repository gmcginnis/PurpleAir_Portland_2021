---
title: "summer_api_scripting"
author: "Gillian McGinnis"
date: "6/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r fixing_qmplot, eval = FALSE}
# This chunk only needs to be run once, upon initial setup
detach("package:ggmap")
remove.packages("ggmap")

devtools::install_github("dkahle/ggmap")

.rs.restartR()
library(ggmap)
```

```{r libraries}
library(tidyverse)
library(AirSensor)
library(lubridate)
library(ggmap)
library(viridis)
library(ggrepel)
```


```{r inputs}
### REQUIRED inputs

# REQUIRED: Start and end dates of interest. Please use the following format: "YYYY-MM-DD", with leading zeros where appropriate
# Data will be pulled from the start of the start date through the end of the end date
input_startdate <- "2021-07-01"
input_enddate <- "2021-07-07"

## REQUIRED: Inside/Outside argument. At least one of the following must be TRUE.
# Grab data for outdoor sensors?
include_outside <- TRUE
# Grab data for indoor sensors?
include_inside <- TRUE

# Drop monitors flagged as reporting high values?
input_drop_hi <- TRUE


### RECOMMENDED (but optional) inputs
# The following are optional but recommended. Set to NULL if not interested in filtering by the following.

## INPUTS FOR GETTING PAS & PAT
# State of monitors to include. If NULL/unspecified, all monitors specified in the above boundaries will be included regardless of state
input_stateCode <- "OR"

# Boundaries (in degrees) of monitors to include. It can also evaluate n/s or e/w pairs of bounds individually.
# Helpful website: bboxfinder [dot] com
input_west <- -122.854
input_south <- 45.4
input_east <- -122.58
input_north <- 45.6

# Labels of monitors to include. If NULL/unspecified, all monitors will be included.
# The labels are based on string detection, so for instance having "STAR" will pull all that have the word "STAR" in the label.
# Example format below. Capitalization matters!
# \\b is added where we want only values that report those words on their own (ex. STAR-LAB and not MYSTARSYSTEM1)
input_labels <- c("se", "SE", "Se", "\\bSTAR\\b", "\\bPSU\\b")
# input_labels <- NULL


## Optional grouping settings (which will NOT impact raw data output, but can be useful when graphing)

## DATE GROUPING
# Run date grouping? If set to "FALSE", the date inputs below will not matter (but will still appear in the environment)
run_date_grouping <- TRUE
# Date grouping categories:
input_date_tags <- c("Before", "Independence Day", "After")
# Start dates (in a list, "YYYY-MM-DD")
input_date_starts <- c("2021-07-01", "2021-07-04", "2021-07-05")
# End dates (in a list, "YYYY-MM-DD")
input_date_ends <- c("2021-07-03", "2021-07-04", "2021-07-07")

## HOUR GROUPING
# Run hour grouping? If set to "FALSE", the hour inputs below will not matter (but will still appear in the environment)
run_hour_grouping <- TRUE
# Hour grouping categories:
input_hour_tags <- c("Morning", "Afternoon", "Evening", "Night")
# Start hours (in a list, using full hours in 24 hour format)
input_hour_starts <- c(5, 12, 17, 21)
# End hours will be automatically generated

# Degrees Fahrenheit difference between reported temperature (raw data) and ambient temperature
# This will not impact correction factors, but will allow for a corrected temperature output (temperature_ambient)
temperature_change <- 8
```


```{r getting_pas}
setArchiveBaseUrl("http://data.mazamascience.com/PurpleAir/v1")

get_area_pas <- function(state_code = input_stateCode,
                         west = input_west, east = input_east, south = input_south, north = input_north,
                         labels = input_labels,
                         datestamp = input_enddate, startdate = input_startdate){
  
  setArchiveBaseUrl("http://data.mazamascience.com/PurpleAir/v1")
  
  # Stringing the selected labels as one argument to be used as a string
  labels_string <- paste0("(", paste(labels, collapse = ")|("), ")")
  
  lookback_days <- as.numeric(as.Date(datestamp) - as.Date(startdate)) + 1
  
  pas <- pas_load(
    # archival allows for retrieval of sensor info even if they are no longer reporting
    archival = TRUE,
    # datestamp is important when loading historical data, as not all monitors might be actively reporting anymore
    datestamp = str_replace_all(as.character(datestamp), "-", ""),
    # Retries (default: 30 days) are the maximum number of days to go back and try to load data if the requested date cannot be retrieved
    retries = lookback_days
  )
  
  # Filtering (or not) filtering by state
  if (is.null(state_code) == TRUE) {
    pas_state <- pas
    print("Sensors not filtered for a specified state code.")
  } else {
    pas_state <- pas_filter(pas, stateCode == state_code)
    print(paste("Sensors now filtered for the state of", state_code))
  }
  
  pas_area <- pas_filterArea(pas = pas_state, w = west, e = east, s = south, n = north) %>% 
    pas_filter(str_detect(label, labels_string))
  
  return(pas_area)
}

pas_area <- get_area_pas()
```

```{r eclean_pas}
# Cleaning environment
remove(get_area_pas, input_stateCode, input_north, input_east, input_south, input_west, input_labels)
```

```{r getting_ids}
ids_outside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = TRUE)
ids_inside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = FALSE)

if (include_outside == TRUE & include_inside == TRUE) {
  ids <- c(ids_outside, ids_inside)
  inout_report <- print("Both indoor and outdoor sensor data will be loaded.")
}
if (include_outside == TRUE & include_inside == FALSE) {
  ids <- ids_outside
  inout_report <- print("Only outdoor sensor data will be loaded.")
}
if (include_outside == FALSE & include_inside == TRUE) {
  ids <- ids_inside
  inout_report <- print("Only indoor sensor data will be loaded.")
}
if (include_outside == FALSE & include_inside == FALSE) {
  inout_report <- warning("INPUT ERROR: Please set `include_outside` and/or `include_inside` to TRUE.")
  stop(inout_report)
}

# Message with feedback as to which monitors will be selected
inout_report

# Warning message if there are many monitors
if (length(ids) > 100) {
  warning("CAUTION: More than 100 monitors selected. Data processing will take significant time. Consider applying more area filters first!")
}
# Message returning the number of monitors that will be selected
print(paste("Number of selected monitors:", length(ids)))
```

```{r eclean_ids}
remove(ids_outside, ids_inside, inout_report)
```

```{r getting_pat}
## Setup for iteration
# URL to be used to grab data
input_baseUrl <- "https://api.thingspeak.com/channels/"
# Number of IDs to be evaluated. Used for console messages.
id_count <- length(ids)
# Starting the counts at 0; will grow upon iteration
count <- 0 
# Creating empty objects to be filled upon iteration appends/row-binds
pat_single <- list()
raw_meta <- data.frame()
data_a <- data.frame()
data_b <- data.frame()
# Columns of interest
input_cols_to_select <- c("created_at", "temperature", "humidity", "pm2.5_cf1", "pm2.5_atm")

input_enddate_1 <- as.Date(input_enddate) + 1
# Sequencing dates into a list by 1 week gaps due to API limitations
date_sequence <- seq(from = as.Date(input_startdate), to = input_enddate_1, by = "week")
# Adding end date to the end of the sequence if not already present in the set
if (tail(date_sequence, n=1) != input_enddate_1) {
  date_sequence <- date_sequence %>%
    append(input_enddate_1)
}
remove(input_enddate_1)

# Iteration
for (id in ids[1:id_count]) {
  
  # Message to report progress
  count <- count + 1
  print(sprintf("Working on %s (%d/%d) ...", id, count, id_count))
  print(sprintf("... from %s to %s", date_sequence[1], date_sequence[2]))
  
  # Creating single PAT for first date range
  pat_single <- pat_downloadParseRawData(
    id = id,
    label = NULL,
    pas = pas_area,
    startdate = date_sequence[1],
    enddate = date_sequence[2],
    baseUrl = input_baseUrl
  )
  
  data_a_single <- pat_single$A_PRIMARY %>% 
    select(intersect(input_cols_to_select, names(.))) %>% 
    mutate("site_id" = id)
  
  data_b_single <- pat_single$B_PRIMARY %>%
    select(intersect(input_cols_to_select, names(.))) %>% 
    mutate("site_id" = id)
  
  # If there is more than a single pair of dates in the date sequence, sub-iterations will occur
  if (length(date_sequence) > 2) {
    
    for (single_position in 2:(length(date_sequence)-1) ) {
      
      print(sprintf("... from %s to %s", date_sequence[single_position], date_sequence[single_position+1]))
      
      pat_single_more <- pat_downloadParseRawData(
        id = id,
        label = NULL,
        pas = pas_area,
        startdate = date_sequence[single_position],
        enddate = date_sequence[single_position + 1],
        baseUrl = input_baseUrl
      )
      
      data_a_more <- pat_single_more$A_PRIMARY %>% 
        select(intersect(input_cols_to_select, names(.))) %>% 
        mutate("site_id" = id)
      
      data_b_more <- pat_single_more$B_PRIMARY %>% 
        select(intersect(input_cols_to_select, names(.))) %>% 
        mutate("site_id" = id)
      
      data_a_single <- rbind(data_a_single, data_a_more)
      data_b_single <- rbind(data_b_single, data_b_more)
      
      # Cleaning environment
      remove(pat_single_more, data_a_more, data_b_more, single_position)
    }
  }
  
  raw_meta_single <- pat_single$meta %>% 
    drop_na(DEVICE_LOCATIONTYPE) %>% 
    mutate("site_id" = id)
  
  raw_meta <- rbind(raw_meta, raw_meta_single)
  data_a <- rbind(data_a, data_a_single)
  data_b <- rbind(data_b, data_b_single)
  
  # Cleaning environment
  remove(data_a_single, data_b_single, pat_single, raw_meta_single)
}
```

```{r eclean_iteration}
remove(count, id, ids, input_baseUrl, input_cols_to_select, date_sequence)
```

```{r pat_joining}
# Joining data frames
raw_data <- full_join(data_a, data_b, # Data sets to join
                      by = c("created_at", "site_id"), # Columns to unite
                      suffix = c("_A", "_B")) %>% # Adding custom suffixes to differentiate the columns
  # Rounding for low time increments, to allow for mostly parallel A & B reports
  mutate(datetime = floor_date(created_at, unit = "2 minutes")) %>% 
  # Grouping for the summarization that will follow
  group_by(site_id, datetime) %>% 
  # Summarizing all numeric columns. Removing NAs prevents values being removed for having an NA in the group
  summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
  # Removing periods from column headers
  rename_with(~gsub("\\.", "", .x))

# Review
print(sprintf("Successfully created %d/%d pat objects.", length(unique(raw_data$site_id)), id_count))
remove(id_count)
```

```{r eclean_joining}
# Already have both saved in raw_data
remove(data_a, data_b)
```

```{r function_tz}
# Time zones are reported as a variable when downloading the data. Time stamps are reported in UTC. The following will convert the time stamps to said reported time zone.
# If more than one time zone is reported in the data, the conversion will use the time zone most frequently used in the data set. This is because no more than one time zone can be applied to a date time variable.
adjust_timezone <- function(dataset, location_data = raw_meta){
  
  # Translation: If more than 1 unique timezone is reported in the data frame, then:
  if (length(unique(location_data$timezone)) > 1) {
  print("Multiple time zones reported. Timestamp will be based on the most frequent time zone reported.")
  timezone <- (location_data %>% 
                 group_by(timezone) %>% 
                 count() %>% 
                 arrange(desc(n)) %>% 
                 pull(timezone))[1]
  } else {
    timezone <- unique(location_data$timezone)
  }
  
  dataset <- dataset %>%
    # Original time stamps (in UTC) will be preserved
    rename(datetime_utc = datetime) %>% 
    mutate(datetime = with_tz(datetime_utc, tzone = timezone))
  
  print(paste("Time zone applied:", timezone))
  return(dataset)
}
```

```{r function_dt}
column_dt <- function(dataset, unit){
  
  if (
    "datetime" %in% colnames(dataset) == FALSE &
    "date_hour" %in% colnames(dataset) == FALSE
    ) { stop("ERROR: columns `datetime` and `date_hour` not found in dataset. Execusion halted.") }
  
  for (unit_single in unit) {
    if (unit_single %in% c("date", "date_hour", "hour", "hour_minute", "time") == FALSE) {
      stop(paste0("Unrecognized unit: \`", unit_single, "\`. Please use `date`, `date_hour`, `hour`, `hour_minute`, or `time`."))
    }
  }
  
  if ("date" %in% unit) {
    if ("date_hour" %in% colnames(dataset) == TRUE) {
      dataset <- dataset %>% 
        mutate(date = date(date_hour))
      print("`date` column added from `date_hour`")
    } else {
      dataset <- dataset %>%
        mutate(date = date(datetime))
      print("`date` column added")
    }
  }
  
  if ("date_hour" %in% unit) {
    dataset <- dataset %>% 
      mutate(date_hour = floor_date(datetime, unit = "hour"))
    print("`date_hour` column added")
  }
  
  if ("time" %in% unit) {
    dataset <- dataset %>% 
      mutate(time = hms::as_hms(datetime))
    print("`time` column added")
  }
  
  if ("hour" %in% unit) {
    if ("date_hour" %in% colnames(dataset) == TRUE) {
      dataset <- dataset %>% 
        mutate(hour = hms::as_hms(date_hour))
      print("`hour` column added")
    } else {
      dataset <- dataset %>% 
        mutate(
          date_hour = floor_date(datetime, unit = "hour"),
          hour = hms::as_hms(date_hour)
        ) %>% 
        select(!date_hour)
      print("`hour` column added, `date_hour` column created then disgarded")
    }
  }
  
  if ("hour_minute" %in% unit) {
    dataset <- dataset %>%
      mutate(
        date_minute = floor_date(datetime, unit = "minute"),
        hour_minute = hms::as_hms(date_minute)
      ) %>%
      select(!date_minute)
    print("`hour_minute` column added, `date_minute` column created then disgarded")
  }
  
  return(dataset)
}
```

```{r function_qc}
apply_qc <- function(dataset, avg_ab = TRUE){
  
  # Creating columns to average A & B data
  if (avg_ab == TRUE) {
    dataset <- dataset %>% 
      rowwise() %>% 
      mutate(
        pm25_cf1 = mean(c(pm25_cf1_A, pm25_cf1_B), na.rm = TRUE),
        pm25_atm = mean(c(pm25_atm_A, pm25_atm_B), na.rm = TRUE)
      ) %>% 
      ungroup()
    print("Columns for averages of A & B data added")
  }
  
  dataset <- dataset %>% 
    # Basic quality control; only physically possible values (and real numbers) are kept
    filter_at(
      # Selecting the columns that start with 'pm25'
      vars(starts_with("pm25")),
      # Filtering said columns such that only values 0:2000 are kept
      all_vars(between(., 0, 2000))
    ) %>% 
    filter(
      # Filtering temperature for -40:185
      between(temperature, -40, 185),
      # Filtering humidity for 0:100
      between(humidity, 0, 100)
    )
  
  return(dataset)
}
```

```{r function_temperature}
ambient_temperature <- function(dataset, variable = temperature, change = temperature_change) {
  
  var_qt <- deparse(substitute(variable))
  
  if(var_qt %in% colnames(dataset) == TRUE & str_detect(var_qt, "_c") == FALSE) {
    dataset <- dataset %>% 
      mutate(temperature_ambient = {{variable}} - change)
    print(paste("Ambient temperature column added using an adjustment of", change))
  } else { print("Temperature not detected, or in degrees Celsius. Please rename the variable or specify it manually.") }
  return(dataset)
}
```

```{r data_qc}
# Basic subsetting and quality control

data_meta <- raw_meta %>% 
  # Selecting only variables of interest, to save on storage
  select(site_id, DEVICE_LOCATIONTYPE, label, longitude, latitude, timezone, flag_highValue)

data_pm25 <- raw_data %>% 
  # Applying quality control (see above function)
  apply_qc() %>% 
  # Adjusting to be reported time zone (see above function)
  adjust_timezone() %>% 
  ambient_temperature() %>% 
  # Reordering variables for ease of reading
  select(site_id, datetime, everything())

if (input_drop_hi == TRUE) {
  hi_monitors <- data_meta %>% 
    filter(flag_highValue == TRUE)
  
  print("Monitors flagged as high value to be filtered out:")
  print(hi_monitors$label)
  
  data_pm25 <- data_pm25 %>% 
    filter(!site_id %in% hi_monitors$site_id)
  
  remove(hi_monitors)
}
```

```{r eclean_qc, eval=FALSE}
remove(apply_qc, adjust_timezone, ambient_temperature)
```

```{r function_epa}
apply_epa <- function(dataset, by_day = TRUE, by_hour = FALSE, epa_percent = 75, keep_cols = FALSE) {
  
  if (by_day == FALSE & by_hour == FALSE) { stop("INPUT ERROR: Please set `by_day` and/or `by_hour` to TRUE.") }
  
  if (by_day == TRUE & by_hour == FALSE) {
    print("Grouping by date (24 hour averages, by day) [default]")
    
    dataset_stamped <- dataset %>% 
      column_dt(c("date", "date_hour"))
    
    groupings_drop <- vars(site_id, date, date_hour)
    groupings <- vars(site_id, date)
    time_unit <- 24
  }
  if (by_day == TRUE & by_hour == TRUE) {
    print("Grouping by date and hour (1 hour averages, by day)")
    
    dataset_stamped <- dataset %>% 
      column_dt(c("date_hour", "hour_minute"))

    groupings_drop <- vars(site_id, date_hour, hour_minute)
    groupings <- vars(site_id, date_hour)
    time_unit <- 60/2
  }
  if (by_day == FALSE & by_hour == TRUE) {
    print("Grouping by hour (1 hour averages)")
    
    dataset_stamped <- dataset %>% 
      column_dt(c("hour", "hour_minute"))
    
    groupings_drop <- vars(site_id, hour, hour_minute)
    groupings <- vars(site_id, hour)
    
    time_unit <- 60/2
  }
  
  # Calculating the minimum number of data points required to be included in the set
  count_to_drop <- ceiling(time_unit*(epa_percent/100))
  
  # Creating a data frame of groups to be removed due to low data quantity
  drop_quantity <- dataset_stamped %>% 
    group_by_at(groupings_drop) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    group_by_at(groupings) %>% 
    count() %>% 
    filter(n < count_to_drop) %>% 
    arrange(n)
  
  print("Values to be dropped via an anti-join due to low data quantity:")
  print(drop_quantity)
  
  drop_ab <- dataset %>% 
    select(!intersect(c("temperature", "humidity", "pm25_cf1", "pm25_atm"), colnames(.))) %>% 
    column_dt("date") %>% 
    group_by(site_id, date) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    mutate(
      # Difference between A & B sensors
      diff = pm25_cf1_A-pm25_cf1_B,
      # Percentage difference between A & B sensors
      per_diff = 100*(abs(pm25_cf1_A-pm25_cf1_B))/((pm25_cf1_A+pm25_cf1_B)/2),
      drop = case_when(
        # Will drop the following based on EPA recommendations
        abs(diff) >= 5 & abs(per_diff) >= 62 ~ TRUE,
        # Fills all other values (the ones to be kept) with 'FALSE'
        TRUE ~ FALSE
        )
      ) %>% 
    filter(drop == TRUE) %>% 
    select(site_id, date)
  
  print("Days (by sensor) to be dropped via an anti-join due to A & B sensor disagreement:")
  print(drop_ab)
  
  dataset_stamped <- dataset_stamped %>% 
    anti_join(drop_quantity) %>% 
    anti_join(drop_ab) %>% 
    group_by_at(groupings) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      # US-wide correction factor published in 2020
      pm25_epa_2020 = 0.524*pm25_cf1 - 0.0852*humidity + 5.72,
      # US-wide correction factor published in late 2020
      pm25_epa_2021 = case_when(
        pm25_cf1 <= 343 ~ 0.52*pm25_cf1 - 0.086*humidity + 5.75,
        pm25_cf1 > 343 ~ 0.46*pm25_cf1 + (3.93*10^(-4))*(pm25_cf1^2) +2.97
      ),
      # US-wide correction factor published in late 2020 using CF=ATM values
      epa_atm = case_when(
        pm25_atm < 50 ~ 0.25*pm25_atm - 0.086*humidity + 5.75,
        pm25_atm >= 50 & pm25_atm < 229 ~ 0.786*pm25_atm - 0.086*humidity + 5.75,
        pm25_atm > 229 ~ 0.69*pm25_atm + (8.84*10^-4)*(pm25_atm^2) + 2.97
      )
    ) %>% 
    # Setting negative values to NA
    mutate_at(vars(pm25_epa_2020, pm25_epa_2021, epa_atm), ~replace(., which(.<0), NA))
  
  if (keep_cols == FALSE) {
    print("Dropping extraneous columns [default]")
    dataset_stamped <- dataset_stamped %>% 
      select(!intersect(c("temperature", "humidity", "pm25_cf1_A", "pm25_atm_A", "pm25_cf1_B", "pm25_atm_B", "pm25_cf1", "pm25_atm"), colnames(.)))
  } else { print("Keeping all columns") }
  
  return(dataset_stamped)
}
```

```{r function_lrapa}
apply_lrapa <- function(dataset, by_day = TRUE, by_hour = FALSE, keep_cols = FALSE) {
  
  if (by_day == FALSE & by_hour == FALSE) {
    stop("INPUT ERROR: Please set `by_day` and/or `by_hour` to TRUE.")
  }
  if (by_day == TRUE & by_hour == FALSE) {
    print("Grouping by date (24 hour averages, by day) [DEFAULT]")
    dataset <- dataset %>% column_dt("date")
    groupings <- vars(site_id, date)
  }
  if (by_day == TRUE & by_hour == TRUE) {
    print("Grouping by date and hour (1 hour averages, by day)")
    dataset <- dataset %>% column_dt("date_hour")
    groupings <- vars(site_id, date_hour)
  }
  if (by_day == FALSE & by_hour == TRUE) {
    print("Grouping by hour (1 hour averages)")
    dataset <- dataset %>% column_dt("hour")
    groupings <- vars(site_id, hour)
  }
  
  dataset <- dataset %>% 
    group_by_at(groupings) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      pm25_lrapa = case_when(pm25_cf1 <= 65 ~ 0.5 * pm25_atm - 0.66)
      ) %>% 
    # Setting negative values to NA
    mutate_at(vars(pm25_lrapa), ~replace(., which(.<0), NA))
  
  if (keep_cols == FALSE) {
    print("Dropping extraneous columns [default]")
    dataset <- dataset %>% 
      select(!intersect(c("temperature", "humidity", "pm25_cf1_A", "pm25_atm_A", "pm25_cf1_B", "pm25_atm_B", "pm25_cf1", "pm25_atm"), colnames(.)))
  } else { print("Keeping all columns") }
  
  return(dataset)
}
```



```{r function_corrections}
apply_corrections <- function(dataset, daily = TRUE, hourly = FALSE, epa = 75){
  epa <- apply_epa(dataset, by_day = daily, by_hour = hourly, epa_percent = epa, keep_cols = TRUE)
  print("EPA corrections applied")
  lrapa <- apply_lrapa(dataset, by_day = daily, by_hour = hourly, keep_cols = TRUE)
  print("LRAPA corrections applied")
  
  dataset_corrected <- full_join(epa, lrapa)
  print("Data frame of corrected values created")
  return(dataset_corrected)
}
```

```{r function_tag_dates}
apply_date_tags <- function(dataset,
                        starts = input_date_starts, ends = input_date_ends, tags = input_date_tags,
                        date_stamp = "17 Jan 1999"){
  
  starts <- as.Date(starts)
  ends <- as.Date(ends)
  tags <- factor(tags)
  date_format <- stamp_date(date_stamp)
  
  data_temp <- data.frame(starts, ends, tags) %>% 
    arrange(starts) %>% 
    mutate(
      start_stamp = date_format(starts),
      end_stamp = date_format(ends),
      date_tag = fct_inorder(paste0(tags, "\n(", start_stamp, " - ", end_stamp, ")"))
    ) %>% 
    pivot_longer(cols = c(starts, ends), values_to = "date") %>% 
    group_by(date_tag) %>% 
    complete(date = full_seq(date, 1)) %>% 
    select(!c(name, tags, start_stamp, end_stamp))
  
  remove_date <- FALSE
  
  if ("date" %in% colnames(dataset) == FALSE) {
    print("`date` column not detected; temporarily adding to dataframe")
    dataset <- column_dt(dataset, "date")
    remove_date <- TRUE
  }
  
  dataset <- dataset %>% 
    left_join(data_temp) %>% 
    drop_na(date_tag)
  
  if (remove_date == TRUE) {
    dataset <- select(dataset, !date)
    print("Temporary `date` column removed")
  }
  
  return(dataset)
}
```

```{r function_tag_hours}
apply_hour_tags <- function(dataset, starts = input_hour_starts, tags = input_hour_tags){
  
  
  if ("hour" %in% colnames(dataset) == FALSE & "date_hour" %in% colnames(dataset) == FALSE) {
    stop("Data set does not contain hours. Execution halted.")
  }
  
  if ("hour" %in% colnames(dataset) == FALSE) {
    print("`hour` column not detected; temporarily adding to dataframe")
    dataset <- column_dt(dataset, "hour")
    remove_hour <- TRUE
  } else { remove_hour <- FALSE }
  
  tags <- factor(tags)
  
  hours_in_day <- data.frame(starts = 0:23)
  
  data_temp <- data.frame(starts, tags) %>% 
    mutate(
      ends = lead(starts - 1),
      ends = replace_na(ends, starts[1]-1),
      tags = paste0(tags, "\n(", formatC(starts, width=2, flag=0), ":00-", formatC(ends, width=2, flag=0), ":59)")
    ) %>% 
    arrange(starts) %>% 
    mutate(hour_tag = fct_inorder(tags)) %>%
    complete(starts = full_seq(starts, 1)) %>%
    right_join(hours_in_day) %>%
    rename(hour = starts) %>%
    fill(hour_tag) %>%
    select(!c(tags, ends)) %>%
    mutate(hour = hms::as_hms(hour*60*60))
  
  dataset <- dataset %>% 
    left_join(data_temp)
  
  if (remove_hour == TRUE) {
    dataset <- select(dataset, !hour)
    print("Temporary `hour` column removed")
  }
  
  return(dataset)
}
```

```{r function_applying}
apply_functions <- function (dataset, by_day = TRUE, by_hour = FALSE, tag_dates = run_date_grouping, tag_hours = run_hour_grouping) {
 
  dataset <- apply_corrections(dataset, daily = by_day, hourly = by_hour)
  print("Correction factors applied.")
  
  if (by_day == TRUE & tag_dates == TRUE) {
    dataset <- apply_date_tags(dataset)
    print("Data now tagged by provided date groupings")
  } else {
    print("Date groups not applied")
  }
  
  if (by_hour == TRUE & tag_hours == TRUE) {
    dataset <- apply_hour_tags(dataset)
    print("Data now tagged by provided hour groupings")
  } else {
    print("Hour groups not applied")
  }
  
  return(dataset)
}
```

```{r data_applying}
data_hourly <- apply_functions(data_pm25, by_day = TRUE, by_hour = TRUE)
data_daily <- apply_functions(data_pm25, by_day = TRUE, by_hour = FALSE)
data_diurnal <- apply_functions(data_pm25, by_day = FALSE, by_hour = TRUE)
```

```{r eclean_applying, eval=FALSE}
remove(apply_corrections, apply_functions, apply_epa, apply_lrapa, apply_hour_tags, apply_date_tags)
remove(list = c(ls(pattern = "input_(hour|date)_")))
```

```{r function_dt_scale}
settings_dt_scale <- function(dataset, start_date = input_startdate, end_date = input_enddate) {
  # Expanding axis to allow monitor labels to be closer to the data
  options_expand <- expansion(mult = c(0.01, 0.01))
  x_angle <- 30
  lab_title_sub <- "across time"
  
  # Setting axis breaks based on the minimum time unit in the data set
  if ("datetime" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", unaveraged")
    dataset <- dataset %>% rename(timestamp = datetime)
    x_scale <- scale_x_datetime(breaks = "1 day", date_labels = "%d %b", expand = options_expand)
    print("Raw set detected: x-axis will map across in units of apx. 2 minutes, with axis breaks each day")
    date_in_set <- TRUE
  } else if ("date_hour" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", averaged hourly by day")
    dataset <- dataset %>% rename(timestamp = date_hour)
    x_scale <- scale_x_datetime(breaks = "1 day", date_labels = "%d %b", expand = options_expand)
    print("Hourly set detected: x-axis will map across in units of hour in each day, with axis breaks each day")
    date_in_set <- TRUE
  } else if ("date" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", averaged by day")
    dataset <- dataset %>% rename(timestamp = date)
    x_scale <- scale_x_date(breaks = "1 day", date_labels = "%d %b", expand = options_expand)
    print("Daily set detected: x-axis will map across in units of 24 hours, with axis breaks each day")
    date_in_set <- TRUE
  } else if ("hour" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", averaged by hour of day")
    dataset <- dataset %>% mutate(time = hms::as_hms(hour), timestamp = as_datetime(time))
    x_scale <- scale_x_datetime(breaks = "1 hour", date_labels = "%H:%M", expand = options_expand)
    x_angle <- 45
    print("Diurnal set detected: Data will map across by hour of day, with axis breaks each hour")
    print("Dates in the caption will default to the inputted start and end dates")
    date_in_set <- FALSE
  } else { stop("INPUT ERROR: No time-based variables found!") }
  
  if (date_in_set == TRUE) {
    start_date <- min(date(dataset$timestamp))
    end_date <- max(date(dataset$timestamp))
  } else {
    start_date <- as.Date(start_date)
    end_date <- as.Date(end_date)
  }
  # Formatting dates
  start_date <- format(start_date, "%d %b %Y")
  end_date <- format(end_date, "%d %b %Y")
  
  lab_caption <- paste0("Data from ", start_date, " through ", end_date, ".")
  
  return(list(
    dataset = dataset,
    lab_title_sub = lab_title_sub,
    x_angle = x_angle,
    x_scale = x_scale,
    date_in_set = date_in_set,
    start_date = start_date,
    end_date = end_date,
    lab_caption = lab_caption
  ))
}
```

```{r function_units}
settings_units <- function(dataset = dataset, var_qt = variable_of_interest_qt,
                           cap_value = NA, cap_color = "red", digits = 2,
                           lab_title = "Graph of", lab_fill = "Units", lab_unit = "units") {
  
  # Defaults
  lab_title_val <- var_qt
  # Color scale will default set to start at 0
  fill_colors <- scale_fill_viridis(option = "plasma", limits = c(0, NA), na.value = cap_color)
  
  if (str_detect(var_qt, "pm") == TRUE) {
    lab_title_val <- "Particulate Matter (PM)"
    lab_unit <- '*mu*"g/m"^3*'
    
    if (str_detect(var_qt, "(25)|(2.5)") == TRUE) {
      pm_val <- 2.5
      print("PM 2.5 detected")
    } else if (str_detect(var_qt, "(1.0)|(01)|(\\D1$)|(1\\D)") == TRUE) {
      pm_val <- 1.0
      print("PM 1.0 detected")
    } else if (str_detect(var_qt, "(10)") == TRUE) {
      pm_val <- 10
      print("PM 10 detected")
    } else {
      pm_val <- NULL
      print("PM unit undetermined")
    }
    lab_fill <- parse(text = paste0('PM[', pm_val, ']~"("', lab_unit,'")"'))
    lab_unit <- "units"
  } else if (str_detect(var_qt, "((H|h)umid)|(rh)|(RH)") == TRUE) {
    # Adjusting color scale and labels if the variable of interest is RH
    lab_title_val <- "humidity"
    lab_unit <- "%"
    lab_fill <- paste0("Relative humidity (", lab_unit, ")")
    fill_colors <- scale_fill_viridis(option = "mako", direction = -1, limits = c(0, 100), end = 0.9, na.value = cap_color)
    print("RH detected as variable of interest; adjusting labels accordingly")
  } else if (str_detect(var_qt, "temp") == TRUE) {
    # Adjusting color scale and labels if the variable of interest is internal temperature
    lab_title_val <- "internal temperature"
    fill_colors <- scale_fill_viridis(option = "cividis", begin = 0.15, na.value = cap_color)
    print("Temperature detected as variable of interest; adjusting labels accordingly")
    lab_unit <- "\u00B0F"
    
    if (str_detect(var_qt, "_c") == TRUE) {
      lab_unit <- "\u00B0C"
      print("Temperature detected to be in Celsius")
    } else { print("Temperature assumed to be in Fahrenheit") }
    
    lab_fill <- paste0("Internal temperature (", lab_unit, ")")
    
    if (str_detect(var_qt, "ambient") == TRUE) {
      print("Ambient temperature (not raw/internal) detected")
      lab_title_val <- "ambient temperature"
      lab_fill <- paste0("Ambient temperature (", lab_unit, ")")
    }
  }
  
  # # Adjusting color scale and labels if the variable of interest is internal temperature
  # if (str_detect(var_qt, "temp") == TRUE) {
  #   lab_title_val <- "internal temperature"
  #   fill_colors <- scale_fill_viridis(option = "cividis", begin = 0.15, na.value = cap_color)
  #   print("Temperature detected as variable of interest; adjusting labels accordingly")
  #   lab_unit <- "\u00B0F"
  #   
  #   if (str_detect(var_qt, "_c") == TRUE) {
  #     lab_unit <- "\u00B0C"
  #     print("Temperature detected to be in Celsius")
  #   } else { print("Temperature assumed to be in Fahrenheit") }
  #   
  #   lab_fill <- paste0("Internal temperature (", lab_unit, ")")
  # }
  # 
  # # Adjusting color scale and labels if the variable of interest is RH
  # if (str_detect(var_qt, "((H|h)umid)|(rh)|(RH)") == TRUE) {
  #   lab_title_val <- "humidity"
  #   lab_unit <- "%"
  #   lab_fill <- paste0("Relative humidity (", lab_unit, ")")
  #   fill_colors <- scale_fill_viridis(option = "mako", direction = -1, limits = c(0, 100), end = 0.9, na.value = cap_color)
  #   print("RH detected as variable of interest; adjusting labels accordingly")
  # }
  
  # Getting min and max values from the data set
  val_min <- dataset %>% ungroup() %>% select_at(vars(var_qt)) %>% min(na.rm = TRUE)
  val_max <- dataset %>% ungroup() %>% select_at(vars(var_qt)) %>% max(na.rm = TRUE)
  
  lab_subtitle <- paste0("Variable plotted: ",
                         var_qt,
                         ", with a reported range of ",
                         round(val_min, digits = digits),
                         " to ",
                         round(val_max, digits = digits),
                         " ", lab_unit, ".")
  
  cap_guide <- guides(fill = guide_colorbar(order = 1, barwidth = 10),
                      color = "none")
  
  # If manually applying a max filter value
  if (is.na(cap_value) == FALSE) {
    # Getting number of rows at or above the cap
    nrow_hi <- dataset %>% 
      filter_at(vars({{var_qt}}),  ~.>= cap_value) %>% 
      nrow()
    
    if ((nrow_hi > 0) == TRUE) {
      # Replacing the values above the set max to be NA so that they will be colored differently on the map
      dataset <- dataset %>% 
        mutate_at(vars({{var_qt}}), ~replace(., which(.>={{cap_value}}), NA))
      
      # Updated lab caption to include the filter
      lab_subtitle <- paste0("Color scale manually capped at ",
                             cap_value, " units; all higher values colored ", cap_color,
                             ".\n", lab_subtitle)
      
      # Feedback
      print(paste(
        "Values greater than or equal to",
        {{cap_value}}, "in", {{var_qt}},
        "will be colored", cap_color
      ))
      
      cap_guide <- guides(fill = guide_colorbar(order = 1, barwidth = 10),
                          color = guide_legend(
                            title = paste0(cap_value, "+"),
                            order = 2,
                            title.position = "bottom",
                            title.theme = element_text(size = 10),
                            override.aes = list(color = cap_color, fill = cap_color)
                          ))
    } else {
      print(paste0(
        "No values greater than or equal to ",
        {{cap_value}}, " found in ", {{var_qt}},
        "; color cap will not be applied."
      ))
    }
  }
  
  return(list(
    lab_title = lab_title,
    lab_title_val = lab_title_val,
    lab_subtitle = lab_subtitle,
    lab_fill = lab_fill,
    fill_colors = fill_colors,
    cap_guide = cap_guide
  ))
}
```

```{r function_shapes}
settings_shapes <- function(dataset = dataset, meta = data_meta) {
  
  if("DEVICE_LOCATIONTYPE" %in% colnames(dataset) == FALSE) {
    dataset <- left_join(dataset, meta)
  }
  
  # Creating a list of unique monitor locations
  unique_locs <- unique(dataset$DEVICE_LOCATIONTYPE)
  shape_nrow <- length(unique_locs)
  
  shapes_df <- data.frame(
    "location" = c("outside", "inside", "DEQ", "average"),
    "shape" = c(21, 23, 25, 24)
  ) %>% 
    filter(location %in% unique_locs)
  
  shape_set <- scale_shape_manual(values = setNames(shapes_df$shape, as.character(shapes_df$location)))
  shape_guide <- guides(shape = guide_legend(override.aes = list(fill = "black", color = "black"), nrow = shape_nrow))
  
  return(list(
    shape_set = shape_set,
    shape_guide = shape_guide
  ))
}
```

```{r function_filtering}
filter_df <- function(dataframe, var = label, include = c(""), exclude = c(""), location_data = data_meta) {
  
  to_include <- paste0("(", paste(include, collapse = ")|("), ")")
  to_exclude <- paste0("(", paste(exclude, collapse = ")|("), ")")
  
  df <- dataframe %>% left_join(data_meta)
  
  df <- df %>% 
    filter(str_detect({{var}}, to_include))
  
  if (to_exclude != "()"){
    df <- df %>% 
      filter(!str_detect({{var}}, to_exclude))
  }
  
  df %>% 
    select(all_of(names(dataframe)))
}
```

```{r function_rounding}
# INPUT: numeric value(s) to round, and the number of digits to preserve
# OUTPUT: character versions rounded to the number of digits specified, including trailing zeroes
# PURPOSE: When pasting labels, R will not display trailing zeroes; this is a way to work around it
# NOTES: The result can be returned to a numeric class if desired
rounding_w_zeroes <- function(num, input_digits) {
  input_digits <- {{input_digits}}
  
  as.character(
    sprintf(
      paste0("%.", deparse(substitute(input_digits)), "f"),
      round(num, digits = input_digits)
    )
  )
}
```

```{r function_cap}
add_cap <- function(dataset, var_qt, cap_value, cap_color, type = "filter") {
  
  # Defaults
  lab_subtitle_cap <- ""
  cap_guide <- guides(fill = guide_colorbar(order = 1, barwidth = 10),
                      color = "none")
  
  # If manually applying a max filter value
  if (is.na(cap_value) == FALSE) {
    
    nrow_ds <- nrow(dataset)
    
    # Getting number of rows at or above the cap
    nrow_hi <- dataset %>% 
      filter_at(vars({{var_qt}}), ~.>= cap_value) %>% 
      nrow()
    
    if (nrow_hi == 0) {
      print(paste0(
        "No values of ", {{var_qt}}, " are greater than or equal to ",
        {{cap_value}}, "; color cap not applied."
      ))
    } else if (nrow_hi == nrow_ds) {
      print(paste0(
        "All values of ", {{var_qt}}," are greater than or equal to ",
        {{cap_value}}, "; color cap not applied."
      ))
    } else if ((nrow_hi > 0) == TRUE) {
      
      if (type == "filter") {
        # Replacing the values above the set max to be NA so that they will be colored differently
        dataset <- dataset %>% 
          mutate_at(vars({{var_qt}}), ~replace(., which(.>={{cap_value}}), NA))
      } else if (type == "flag") {
        dataset <- dataset %>% 
          mutate(above_cap = case_when(
            !!sym({{var_qt}}) >= {{cap_value}} ~ TRUE,
            !!sym({{var_qt}}) < {{cap_value}} ~ FALSE
          ))
      }
      
      # Updated lab caption to include the filter
      lab_subtitle_cap <- paste0("Color scale manually capped at ",
                                 cap_value, " units; all higher values colored ", cap_color)
      
      # Feedback
      print(paste(
        "Values greater than or equal to",
        {{cap_value}}, "in", {{var_qt}},
        "will be colored", cap_color
      ))
      
      cap_guide <- guides(
        fill = guide_colorbar(
          order = 1,
          barwidth = 10
        ),
        color = guide_legend(
          title = paste0(cap_value, "+"),
          order = 2,
          title.position = "bottom",
          title.theme = element_text(size = 8.5),
          override.aes = list(
            color = cap_color,
            fill = cap_color,
            shape = 22,
            size = 7
          )
        ))
    } else { print("Error.") }
  }
  
  return(list(
    dataset = dataset,
    lab_subtitle_cap = lab_subtitle_cap,
    cap_guide = cap_guide
  ))
}
```

```{r function_drop_incomplete}
drop_incomplete <- function(dataset, var_qt) {
  dataset <- dataset %>% 
    drop_na({{var_qt}})
  
  # Number of values expected for a complete set
  complete_num <- (dataset %>% 
                     ungroup() %>% 
                     count(site_id) %>% 
                     arrange(desc(n)) %>% 
                     pull(n))[1]
  
  # List of monitors with incomplete sets
  to_drop <- dataset %>% 
    ungroup() %>% 
    count(site_id) %>% 
    filter(n != complete_num) %>% 
    pull(site_id)
  
  # Feedback
  print("Monitors with incomplete temporal data that will be dropped:")
  print(paste(to_drop))
  
  # Removing incomplete monitors
  dataset %>% 
    filter(!site_id %in% to_drop)
}
```

```{r viz_function_map}
# FUNCTION
# Required inputs: atmospheric (df), value (column of PM2.5 data w/in df to map), location_data (lat/long data to be joined to df; default: data_meta)
# Optional inputs: grouping variables (1 or 2) & map customization: type, zoom, overlay tint alpha & color, point size

map_q <- function(dataset, variable_of_interest, grouping_vars = NULL, location_data = data_meta,
                  cap_value = NA, cap_color = "red",
                  start_date = input_startdate, end_date = input_enddate,
                  point_size = 3, maptype = "toner-lite", zoom = 11, tint_alpha = 0.5, tint_color = "black"){
  
  # Warning message and halting execution if required input is missing.
  if (missing(variable_of_interest) == TRUE) {
    stop ("ERROR in argument 'variable_of_interest': Missing input, with no default. Execution haulted. Please input a valid PM2.5 variable (e.g. pm25_atm).")
  }
  
  # Warning message and halting execution if inputeded data does not have more than one unique site_id (map will bug)
  if (length(unique(dataset$site_id)) == 1) {
    stop("ERROR: Insufficient data to create a map. Provide a data set with more than one site.")
  }

  variable_of_interest_qt <- deparse(substitute(variable_of_interest))
  
  scale_results <- settings_dt_scale(dataset = dataset, start_date = start_date, end_date = end_date)
  
  lab_title_sub <- scale_results$lab_title_sub
  lab_subtitle <- scale_results$lab_caption
  
  # Wrangling based on provided inputs
  dataset <- dataset %>% 
    ungroup() %>% 
    # Grouping by provided variables, as well as site_id
    group_by_at(vars(site_id, {{grouping_vars}})) %>% 
    # Calculating means by said grouping variables
    summarize(mean = mean({{variable_of_interest}}, na.rm = TRUE)) %>% 
    drop_na(mean) %>% 
    # Adding location data to get lat & lon
    left_join(location_data)
  
  # Renaming newly-created "mean" variable
  names(dataset)[names(dataset) == 'mean'] <- variable_of_interest_qt
  
  # Feedback message
  print("Data now grouped and averaged. Location data added.")
  
  shape_results <- settings_shapes(dataset = dataset, meta = location_data)
  
  shape_set <- shape_results$shape_set
  shape_guide <- shape_results$shape_guide
  
  unit_results <- settings_units(dataset = dataset, var_qt = variable_of_interest_qt,
                                cap_color = cap_color, lab_title = "Map of")
  
  lab_title <- unit_results$lab_title
  lab_title_val <- unit_results$lab_title_val
  lab_subtitle <- paste(lab_subtitle, unit_results$lab_subtitle, sep = "\n")
  lab_fill <- unit_results$lab_fill
  fill_colors <- unit_results$fill_colors
  
  cap_results <- add_cap(dataset = dataset, var_qt = variable_of_interest_qt,
                         cap_value = cap_value, cap_color = cap_color)
  
  dataset <- cap_results$dataset
  lab_subtitle <- paste(lab_subtitle, cap_results$lab_subtitle_cap, sep="\n")
  cap_guide <- cap_results$cap_guide
  
  # Base plot
  plot <- qmplot(data = dataset, x = longitude, y = latitude,
                 geom = "blank", maptype = maptype, zoom = zoom, darken = c(tint_alpha, tint_color)) +
    # facet_wrap(vars({{grouping_var}})) +
    geom_point(
      aes(fill = {{variable_of_interest}}, shape = DEVICE_LOCATIONTYPE, color = ""),
      # color = "white",
      alpha = 0.8,
      size = point_size
    ) +
    fill_colors +
    theme_void() +
    theme(
      # legend.box = "vertical",
      legend.position = "bottom"
    ) + 
    shape_set +
    shape_guide +
    # scale_shape_manual(values = shapes_inout) +
    # guides(shape = guide_legend(override.aes = list(fill="black"), nrow = shape_nrow)) +
    labs(
      title = paste(lab_title, lab_title_val),
      subtitle = lab_subtitle,
      fill = lab_fill,
      shape = "Monitor location:",
      caption = paste0("Data ", lab_title_sub, "; displayed as collective average by location")
    ) +
    cap_guide +
    scale_color_manual(values = "transparent")
  
  # Feedback message
  print("Base plot created.")
  
  if (length(grouping_vars) == 2) {
    plot <- plot +
      facet_grid(
        formula(paste(
          grouping_vars[1],
          "~",
          grouping_vars[2]
        ))) +
      labs(
        caption = paste(
          "Data",
          lab_title_sub,
          "then grouped by",
          grouping_vars[1],
          "and",
          grouping_vars[2]
        )
      )
    # Feedback message
    print(paste("Plot now faceted by",
                grouping_vars[1],
                "and",
                grouping_vars[2]
    ))
  } else if (length(grouping_vars) == 1) {
    plot <- plot +
      facet_wrap(grouping_vars) +
      labs(
        caption = paste(
          "Data",
          lab_title_sub,
          "then grouped by",
          grouping_vars[1]
        )
      )
    # Feedback message
    print(paste("Plot now faceted by", grouping_vars))
  }
  
  print("Final plot created.")
  
  # Returning the final plot
  plot
}
```

```{r test_map, eval=FALSE}
map_q(data_daily, variable_of_interest = pm25_epa_2021, grouping_vars = "date_tag", cap_value = 12, cap_color = "green")
map_q(data_daily, variable_of_interest = pm25_atm, cap_value = 11)
map_q(data_hourly, pm25_atm, cap_value = 17, cap_color = "black", grouping_vars = c("hour_tag", "date_tag"))
```

```{r viz_function_single_heatmap}
heatmap_single <- function(variable_of_interest, site_of_interest,
                           cap_value = NA, cap_color = "red",
                           data_labels = TRUE, digits = 2, date_breaks = "1 day",
                           text_color = "black", dataset = data_hourly, location_data = data_meta){
  
  if ("date_hour" %in% colnames(dataset) == FALSE) { stop("ERROR: Inputted data set is not hourly by day") }
  
  variable_of_interest_qt <- deparse(substitute(variable_of_interest))
  
  if (variable_of_interest_qt %in% colnames(dataset) == FALSE) {
    dataset %>%
      slice(1) %>%
      ungroup() %>%
      select_if(is.numeric) %>%
      colnames() %>%
      print()
    stop("ERROR: Inputted variable of interest is not in the provided data set. Execution halted. Valid inputs are listed above")
  }
  
  temp_loc <- location_data %>% 
    select(site_id, label, DEVICE_LOCATIONTYPE)
  
  dataset <- dataset %>% 
    ungroup() %>% 
    # Adding labels to the data set
    left_join(temp_loc) %>% 
    # Filtering data set based on inputted site of interest
    filter(str_detect(label, site_of_interest)) %>% 
    # Removing empty values from variable of interest (otherwise is grey when mapping)
    drop_na({{variable_of_interest}}) %>% 
    # Removing duplicate rows
    distinct() %>% 
    mutate(val_orig = {{variable_of_interest}})
  
  # dataset_vals <- dataset %>%
  #   select("date_hour", "vals" = variable_of_interest_qt)
  
  # Verification that only one monitor is selected. Execution will halt if 0 or >1 are detected
  if (length(unique(dataset$label)) == 0) {
    stop("ERROR: No monitors selected. Please verify that a distinct label was provided. Capitalization matters.")
  }
  if (length(unique(dataset$label)) > 1) {
    print("Matching locations from meta data:")
    temp_loc %>% filter(str_detect(label, site_of_interest)) %>% distinct() %>% pull(label) %>% print()
    print("Matching locations that contain values of interest:")
    dataset %>% pull(label) %>% unique() %>% print()
    stop("ERROR: More than one site selected. Please use a more precise string argument; matching values are listed above.")
  }
  
  unit_results <- settings_units(dataset = dataset, var = variable_of_interest_qt,
                                 cap_color = cap_color, lab_title = "Heatmap of")
  
  lab_title <- unit_results$lab_title
  lab_title_val <- unit_results$lab_title_val
  lab_subtitle <- paste0(
    "Monitor selected: \"", dataset %>% pull(label),
    "\" (", dataset %>% pull(DEVICE_LOCATIONTYPE),
    ", ID: ", dataset %>% pull(site_id),").\n",
    unit_results$lab_subtitle
  )
  lab_fill <- unit_results$lab_fill
  fill_colors <- unit_results$fill_colors
  
  
  cap_results <- add_cap(dataset = dataset, var_qt = variable_of_interest_qt,
                         cap_value = cap_value, cap_color = cap_color)
  
  dataset <- cap_results$dataset
  lab_subtitle <- paste(lab_subtitle, cap_results$lab_subtitle_cap, sep="\n")
  cap_guide <- cap_results$cap_guide
  
  
  scale_results <- settings_dt_scale(dataset = dataset, start_date = NA, end_date = NA)
  
  dataset <- scale_results$dataset %>% rename(date_hour = timestamp)
  lab_title_sub <- scale_results$lab_title_sub
  lab_caption <- scale_results$lab_caption
  
  
  if (data_labels == TRUE) {
    data_labels <- geom_text(color = {{text_color}})
  } else { data_labels <- NULL }
  
  # Plotting data
  dataset %>% 
    ggplot(aes(
      x = date(date_hour),
      y = hour(date_hour),
      fill = {{variable_of_interest}},
      color = "",
      label = rounding_w_zeroes(val_orig, digits)
    )) +
    geom_tile() +
    fill_colors +
    data_labels +
    scale_y_continuous(
      # Reverse such that morning is at top, night at bottom
      trans = "reverse",
      # Labeling with proper hourly formatting
      labels = function(x) sprintf("%02d:00", x),
      # # Limits to force axis to always display 00 to 24 hr, regardless of availability
      # limits = c(24,-1),
      # Breaks every hour
      breaks = 0:23
    ) +
    scale_x_date(
      breaks = date_breaks,
      expand = c(0.01, 0.01),
      date_labels = "%d %b"
    ) +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      panel.grid = element_blank(),
      axis.title.x = element_blank(),
      axis.text.x = element_text(angle = 30, hjust = 1),
      axis.ticks.x = element_line()
    ) +
    labs(
      title = paste(lab_title, lab_title_val, lab_title_sub),
      subtitle = lab_subtitle,
      fill = lab_fill,
      caption = lab_caption,
      y = "Hour of the day"
    ) + 
    scale_color_manual(values = "transparent") +
    cap_guide
}
```

```{r test_heatmap_single, eval=FALSE}
heatmap_single(pm25_epa_2021, "Sunset")
heatmap_single(pm25_atm, "Sunset")
heatmap_single(pm25_atm, "Sunset", cap_value = 15)
heatmap_single(variable_of_interest = pm25_atm, site_of_interest = "Lighthouse", cap_value = 20, cap_color = "green", data_label = F)
```

```{r viz_function_heatmap}
heatmap_cross <- function(dataset, variable_of_interest, drop_incomplete = FALSE,
                          cap_value = NA, cap_color = "red",
                          data_labels = FALSE, text_color = "black",
                          location_data = data_meta, digits = 2,
                          start_date = input_startdate, end_date = input_enddate){
  
  # Dropping NA values from the variable of interest
  # NA values will appear as gaps if viewing the complete viz, rather than gray
  # Adding another column with duplicate info, in case a color cap applied AND text labeling is desired
  dataset <- dataset %>% 
    drop_na({{variable_of_interest}}) %>% 
    mutate(val_orig = {{variable_of_interest}})
  
  variable_of_interest_qt <- deparse(substitute(variable_of_interest))
  
  if (drop_incomplete == TRUE) {
    dataset <- drop_incomplete(dataset, var_qt = variable_of_interest_qt)
  } else { print("All monitors will be plotted.") }
  
  # Adding quotation marks
  cap_color <- deparse(substitute(cap_color)) %>%
    str_replace_all("\\\"", "") # Removing extra quotation marks if already provided
  
  # Location data
  temp_loc <- location_data %>% 
    # Arranging such that northern-most monitors will be on top
    mutate(label = fct_reorder(as.factor(label), desc(latitude))) %>% 
    # Selecting only variables of interest to save space
    select(site_id, label, DEVICE_LOCATIONTYPE)
  
  unit_results <- settings_units(dataset = dataset, var = variable_of_interest_qt, cap_color = cap_color,
                                 lab_title = "Heatmap of")
  
  lab_title <- unit_results$lab_title
  lab_title_val <- unit_results$lab_title_val
  lab_subtitle <- unit_results$lab_subtitle
  lab_fill <- unit_results$lab_fill
  fill_colors <- unit_results$fill_colors
  
  cap_results <- add_cap(dataset = dataset, var_qt = variable_of_interest_qt,
                         cap_value = cap_value, cap_color = cap_color)
  
  dataset <- cap_results$dataset
  lab_subtitle <- paste(lab_subtitle, cap_results$lab_subtitle_cap, sep="\n")
  cap_guide <- cap_results$cap_guide
  
  scale_results <- settings_dt_scale(dataset = dataset, start_date = start_date, end_date = end_date)
  
  dataset <- scale_results$dataset
  lab_title_sub <- scale_results$lab_title_sub
  x_angle <- scale_results$x_angle
  x_scale <- scale_results$x_scale
  lab_caption <- scale_results$lab_caption
  
  lab_caption <- paste0(lab_caption, "\nMonitors are arranged north to south in their respective groups.")
  
  if (data_labels == TRUE) {
    data_labels <- geom_text(color = {{text_color}})
  } else { data_labels <- NULL }
  
  dataset %>% 
    distinct() %>% 
    left_join(temp_loc) %>% 
    ggplot(
      aes(
        x = timestamp,
        y = label,
        color = "",
        fill = {{variable_of_interest}},
        label = rounding_w_zeroes(val_orig, digits)
      )
    ) +
    facet_grid(
      DEVICE_LOCATIONTYPE~.,
      scales = "free_y",
      space = "free_y"
    ) +
    geom_tile() +
    fill_colors + 
    data_labels +
    scale_y_discrete(limits = rev) + 
    theme_minimal() +
    theme(
      legend.position = "bottom",
      panel.grid = element_blank(),
      panel.border = element_blank(),
      axis.text.x = element_text(angle = x_angle, hjust = 1),
      axis.title = element_blank(),
      axis.ticks.x = element_line()
    ) + 
    labs(
      title = paste(lab_title, lab_title_val, lab_title_sub),
      subtitle = lab_subtitle,
      fill = lab_fill,
      caption = lab_caption
    ) +
    x_scale + 
    scale_color_manual(values = "transparent") +
    cap_guide
}
```

```{r test_heatmap, eval = FALSE}
heatmap_cross(data_hourly, pm25_epa_2021)
heatmap_cross(data_daily, humidity, data_labels = TRUE)
heatmap_cross(data_diurnal, pm25_atm, cap_value = 10, cap_color = "green")
heatmap_cross(data_pm25, pm25_atm, cap_value = 50, cap_color = "green")
```

```{r viz_function_ts}
ts_line <- function(dataset, variable_of_interest,
                    add_extrema = TRUE, digits = 2,
                    add_points = FALSE,
                    add_average = TRUE, 
                    cap_value = NA, cap_color = "green",
                    label_filter = c(""),
                    single_column = FALSE, label_length = 10,
                    avg_color = "darkgreen", min_color = "royalblue", max_color = "darkorange",
                    location_data = data_meta,
                    start_date = input_startdate, end_date = input_enddate){
  
  variable_of_interest_qt <- deparse(substitute(variable_of_interest))
  
  # Dropping NA values from the variable of interest
  dataset <- dataset %>% 
    drop_na({{variable_of_interest}})
  
  location_data <- location_data %>% 
    inner_join((dataset %>% distinct(site_id))) %>% 
    mutate(label = make.unique(label, sep = " "))
  
  if (single_column == TRUE) {
    facet <- facet_grid(label~.)
    print("Charts will be arranged in a single column (multiple rows).")
    location_data <- location_data %>% mutate(label = str_wrap(label, label_length))
    print("Line breaks added to labels")
  } else {
    facet <- facet_wrap(~label)
    print("Charts will be arranged in multiple rows and columns.")
  }
  
  scale_results <- settings_dt_scale(dataset = dataset, start_date = start_date, end_date = end_date)
  
  dataset <- scale_results$dataset
  lab_title_sub <- scale_results$lab_title_sub
  x_angle <- scale_results$x_angle
  x_scale <- scale_results$x_scale
  lab_dates <- scale_results$lab_caption
  lab_caption <- paste("Black lines indiciate the data for a specific monitor of interest.",
                       "Gray lines represent data from all monitors.",
                       "\nMaximum and minimum values by monitor are marked by", max_color,
                       "and", min_color, "text, respectively.")
  
  
  dataset <- dataset %>% 
    left_join(location_data) %>% 
    select(timestamp, site_id, label, DEVICE_LOCATIONTYPE, {{variable_of_interest}})
  
  input_labels <- paste0("(", paste(label_filter, collapse = ")|("), ")")
  
  label_order <- location_data %>% 
    select(site_id, label, latitude) %>% 
    distinct() %>% 
    mutate(label = fct_reorder(as.factor(label), desc(latitude))) %>% 
    pull(label)
  
  extrema <- dataset %>% 
    filter(str_detect(label, input_labels))
  
  if (add_extrema == TRUE) {
    extrema <- extrema %>% 
      group_by(site_id) %>% 
      mutate(
        date = date(timestamp),
        max = case_when({{variable_of_interest}} == max({{variable_of_interest}}, na.rm = TRUE) ~ {{variable_of_interest}}),
        min = case_when({{variable_of_interest}} == min({{variable_of_interest}}, na.rm = TRUE) ~ {{variable_of_interest}})
      ) %>% 
      group_by(site_id, date) %>% 
      mutate_at(vars(max, min), ~replace(., duplicated(.), NA)) %>% 
      ungroup() %>% 
      select(!date)
  }
  
  if (add_average == TRUE) {
    
    extrema_avg <- dataset %>% 
      ungroup() %>% 
      select(timestamp, {{variable_of_interest}}) %>%
      group_by(timestamp) %>% 
      summarize(mean = mean({{variable_of_interest}}, na.rm = TRUE)) %>% 
      mutate(
        site_id = "Averages",
        label = "AVERAGE",
        DEVICE_LOCATIONTYPE = "average"
      )
    
    if (add_extrema == TRUE) {
      extrema_avg <- extrema_avg %>% 
        mutate(
          date = date(timestamp),
          max = case_when(mean == max(mean, na.rm = TRUE) ~ mean),
          min = case_when(mean == min(mean, na.rm = TRUE) ~ mean)
        ) %>%
        group_by(date, site_id, label) %>% 
        mutate_at(vars(max, min), ~replace(., duplicated(.), NA)) %>%
        ungroup() %>% 
        select(!date)
    }
    
    names(extrema_avg)[names(extrema_avg) == 'mean'] <- variable_of_interest_qt
    
    extrema <- rbind(extrema, extrema_avg)
    
    # Appending the "average" label onto the set
    label_order <- fct_inorder(c(levels(label_order), "AVERAGE"))
    
    print("Average data added.")
    
    lab_caption <- paste(
      lab_caption,
      "\nThe", avg_color, "colored graph represents the moving average of all monitors in the data set."
    )
    
  } else {
    label_order <- fct_inorder(levels(label_order))
    print("No averages will be added.")
  }
  
  dataset_full <- dataset %>% 
    select(!label)
  
  dataset <- extrema
  
  unit_results <- settings_units(dataset = dataset, var = variable_of_interest_qt,
                                 cap_color = NA, lab_title = "Timeseries of")

  lab_title <- unit_results$lab_title
  lab_title_val <- unit_results$lab_title_val
  lab_subtitle <- unit_results$lab_subtitle
  lab_fill <- unit_results$lab_fill
  y_lab <- lab_fill
  fill_colors <- unit_results$fill_colors
  
  
  cap_results <- add_cap(dataset = dataset, var_qt = variable_of_interest_qt,
                             cap_value = cap_value, cap_color = cap_color, type = "flag")

  dataset <- cap_results$dataset
  lab_subtitle <- paste(lab_subtitle, cap_results$lab_subtitle_cap, sep="\n")
  cap_guide <- cap_results$cap_guide

  
  if (add_points == TRUE) {
    shape_results <- settings_shapes(dataset = dataset)
    
    shape_set <- shape_results$shape_set
    shape_guide <- shape_results$shape_guide
    
    data_points <- geom_point(
      aes(fill = {{variable_of_interest}}),
      alpha = 0.7,
      # shape = 21,
      stroke = 0
    )
    above_cap_points <- NULL

    if ("above_cap" %in% colnames(dataset)) {
      data_points <- geom_point(
        data = . %>% filter(above_cap == FALSE),
        aes(fill = {{variable_of_interest}}),
        alpha = 0.7,
        # shape = 21,
        stroke = 0
      )
      above_cap_points <- geom_point(
        data = . %>% filter(above_cap == TRUE),
        color = {{cap_color}},
        fill = {{cap_color}},
        # aes(fill = {{variable_of_interest}}),
        alpha = 0.7,
        stroke = 0
        # shape = 16
      )
    }

    print("Data points will be added.")
  } else {
    data_points <- NULL
    above_cap_points <- NULL
    shape_set <- NULL
    shape_guide <- NULL
  }

  if (add_extrema == TRUE) {
    dataset <- dataset %>%
      mutate_at(vars(max, min), ~case_when(!is.na(.) ~ rounding_w_zeroes(., digits))) %>% 
      mutate_at(vars(max, min), ~replace_na(., ""))
  }
  
  plot <- dataset %>%
    mutate(label = factor(as.character(label), label_order)) %>%
    ggplot(aes(
      x = timestamp,
      y = {{variable_of_interest}},
      color = "",
      group = label,
      shape = DEVICE_LOCATIONTYPE
    )) +
    facet +
    geom_line(
      data = dataset_full,
      aes(group = site_id),
      color = "gray",
      alpha = 0.5
    ) +
    geom_line(
      data = . %>% filter(site_id != "Averages"),
      color = "black"
    ) +
    geom_line(
      data = . %>% filter(site_id == "Averages"),
      color = avg_color
    ) +
    data_points +
    above_cap_points +
    fill_colors +
    scale_color_manual(values = "transparent") +
    x_scale +
    shape_set +
    shape_guide +
    cap_guide +
    theme_minimal() +
    theme(
      panel.spacing.x = unit(5, "mm"),
      legend.position = "bottom",
      panel.grid = element_blank(),
      panel.border = element_blank(),
      axis.text.x = element_text(angle = x_angle, hjust = 1),
      axis.title.x = element_blank(),
      axis.ticks.x = element_line()
    ) +
    labs(
      title = paste(lab_title, lab_title_val, lab_title_sub),
      subtitle = paste(lab_subtitle, lab_dates, sep = "\n"),
      caption = lab_caption,
      x = "Time",
      y = y_lab,
      fill = lab_fill,
      shape = "Monitor location:"
    )
  
  if (add_extrema == TRUE) {
    plot <- plot +
      geom_point(
        data = . %>% filter(max != ""),
        color = max_color,
        shape = 1
      ) +
      geom_point(
        data = . %>% filter(min != ""),
        color = min_color,
        shape = 1
      ) +
      geom_text_repel(
        aes(label = max),
        min.segment.length = 0,
        box.padding = 0.75,
        max.overlaps = Inf,
        color = max_color
      ) +
      geom_text_repel(
        aes(label = min),
        min.segment.length = 0,
        box.padding = 0.75,
        max.overlaps = Inf,
        nudge_y = max(dataset %>% pull({{variable_of_interest}}), na.rm = TRUE) / 5,
        color = min_color
      )
  }
  
  return(plot)
}
```

```{r test_ts, eval = FALSE}
ts_line(data_hourly, pm25_atm)
ts_line(data_hourly, temperature, add_points = TRUE)
ts_line(data_hourly, pm25_atm, add_points = TRUE)
ts_line(data_hourly, pm25_atm, label_filter = c("\\bSTAR\\b"), single_column = TRUE)
```

```{r aug_viz, eval=FALSE}
map_q(data_daily, pm25_epa_2021, "date_tag")

heatmap_single(pm25_epa_2021, "Harrison")

data_daily %>% 
  filter_df(., include = c("STAR", "SE", "NCA", "Colby")) %>%
  heatmap_cross(., pm25_epa_2021, drop_incomplete = TRUE, data_labels = TRUE)

heatmap_cross(data_hourly, pm25_epa_2021, drop_incomplete = TRUE, cap_value = 70, cap_color = "green")

ts_line(data_hourly, pm25_epa_2021, label = "STAR", include_avg = FALSE)

ts_line(data_hourly, pm25_epa_2021, label = c("STAR", "Woodlands"), add_points = TRUE)
```


```{r viz_ts_variation}
ts_variation <- function(dataset, pollutants, group,
                         subset = c("hour", "day.hour", "day", "month"),
                         include, exclude,
                         location_data = data_meta,
                         color = "Dark2"){
  if (missing(include) == FALSE) {
    dataset <- filter_df(dataset, include = include, location_data = location_data)
  }
  if (missing(exclude) == FALSE) {
    dataset <- filter_df(dataset, exclude = exclude, location_data = location_data)
  }
  
  if ("date" %in% colnames(dataset) == FALSE) {
    dataset <- (settings_dt_scale(dataset))$dataset %>% rename(date = timestamp)
  }
  
  if (missing(group) == TRUE) {
    plot <- timeVariation(dataset, pollutant = pollutants, cols = color)
  } else {
    if (group %in% colnames(dataset) == TRUE) {
      plot <- timeVariation(dataset, pollutant = pollutants, cols = color, group = group)
    } else {
      print(paste("ERROR: grouping variable", group, "not found in dataset. Plot will not be grouped."))
      plot <- timeVariation(dataset, pollutant = pollutants, cols = color)
    }
  }
  
  if (length(subset) == 1) {
    if(subset %in% c("hour", "day.hour", "day", "month") == FALSE) {
      print("Invalid plotting subset value. Defaulting to plotting all.")
      pluck <- NULL
    } else {
      print(paste("Plot selected:", subset))
      pluck <- subset
    }
  } else { pluck <- NULL }
  remove(subset)
  
  plot_final <- plot %>%
    plot(subset = pluck)
  
  return(plot_final)
}
```

```{r ts_variation_testing}
ts_variation(data_hourly, "pm25_atm", include = "Lighthouse", group = "date_tag")
ts_variation(data_pm25 %>% apply_date_tags(), c("pm25_atm", "pm25_lrapa"), include = "Lighthouse", group = "date_tag")
ts_variation(data_pm25, pollutants= c("pm25_atm", "temperature"), include = "Lighthouse")
```


