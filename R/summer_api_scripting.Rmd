---
title: "summer_api_scripting"
author: "Gillian McGinnis"
date: "6/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, fixing_qmplot, eval=FALSE}
detach("package:ggmap")
remove.packages("ggmap")

devtools::install_github("dkahle/ggmap")

.rs.restartR()
library(ggmap)
```


```{r, libraries}
library(tidyverse)
library(AirSensor)
library(lubridate)
library(ggmap)
library(viridis)
```

```{r, inputs}
# REQUIRED: Start and end dates of interest. Please use the following format: "YYYY-MM-DD", with leading zeros where appropriate
input_startdate <- "2021-07-01"
input_enddate <- "2021-07-06"

## REQUIRED: Inside/Outside argument. At least one of the following must be TRUE
# Grab data for outdoor sensors?
include_outside <- TRUE
# Grab data for indoor sensors?
include_inside <- TRUE

## INPUTS FOR GETTING PAS & PAT
## The following are optional but recommended. Set to NULL if not interested in filtering by the following.
# State of monitors to include. If NULL/unspecified, all monitors specified in the above boundaries will be included regardless of state
input_stateCode <- "OR"

# Boundaries (in degrees) of monitors to include. It can also evaluate n/s or e/w pairs of bounds individually.
input_north <- 45.6
input_south <- 45.4
input_east <- -122.58
input_west <- -122.854

# Labels of monitors to include. If NULL/unspecified, all monitors will be included.
# The labels are based on string detection, so for instance having "STAR" will pull all that have the word "STAR" in the label.
# Use the following format (capitalization matters):
#input_labels <- c("se", "SE", "Se", "STAR", "PSU")
input_labels <- NULL

## DATE GROUPING: group values into tags (will not impact raw data output, but can be useful when graphing)
# Run date grouping?
run_date_grouping <- TRUE
# Date grouping categories:
input_date_tags <- c("Before", "Independence Day", "After")
# Start dates (in a list, "YYYY-MM-DD")
input_date_starts <- c("2021-07-01", "2021-07-04", "2021-07-05")
# End dates (in a list, "YYYY-MM-DD")
input_date_ends <- c("2021-07-03", "2021-07-04", "2021-07-08")

## HOUR GROUPING: group values into tags (will not impact raw data output, but can be useful when graphing)
# Run hour grouping?
run_hour_grouping <- TRUE
# Hour grouping categories:
input_hour_tags <- c("Morning", "Afternoon", "Evening", "Night")
# Start hours (in a list, using full hours in 24 hour format)
input_hour_starts <- c(5, 12, 17, 21)
# End hours will be automatically generated
```


```{r, getting_pas}
get_area_pas <- function(state_code = input_stateCode,
                         west = input_west, east = input_east, south = input_south, north = input_north,
                         labels = input_labels,
                         datestamp = input_enddate, startdate = input_startdate){
  
  setArchiveBaseUrl("http://data.mazamascience.com/PurpleAir/v1")
  
  # Stringing the selected labels as one argument to be used as a string
  labels_string <- paste0("(", paste(labels, collapse = ")|("), ")")
  
  lookback_days <- as.numeric(as.Date(datestamp) - as.Date(startdate)) + 1
  
  pas <- pas_load(
    # archival allows for retrieval of sensor info even if they are no longer reporting
    archival = TRUE,
    # datestamp is important when loading historical data, as not all monitors might be actively reporting anymore
    datestamp = str_replace_all(as.character(datestamp), "-", ""),
    # Retries (default: 30 days) are the maximum number of days to go back and try to load data if the requested date cannot be retrieved
    retries = lookback_days
  )
  
  # Filtering (or not) filtering by state
  if(is.null(state_code) == TRUE){
    pas_state <- pas
    print("Sensors not filtered for a specified state code.")
  } else {
    pas_state <- pas_filter(pas, stateCode == state_code)
    print(paste("Sensors now filtered for the state of", state_code))
  }
  
  pas_area <- pas_filterArea(pas = pas_state, w = west, e = east, s = south, n = north) %>% 
    pas_filter(str_detect(label, labels_string))
  
  return(pas_area)
}

pas_area <- get_area_pas()
```

```{r, eclean_pas}
# Cleaning environment
remove(get_area_pas, input_stateCode, input_north, input_east, input_south, input_west, input_labels)
```

```{r, getting_ids}
ids_outside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = TRUE)
ids_inside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = FALSE)

if(include_outside == TRUE & include_inside == TRUE){
  ids <- c(ids_outside, ids_inside)
  inout_report <- print("Both indoor and outdoor sensor data will be loaded.")
}
if(include_outside == TRUE & include_inside == FALSE){
  ids <- ids_outside
  inout_report <- print("Only outdoor sensor data will be loaded.")
}
if(include_outside == FALSE & include_inside == TRUE){
  ids <- ids_inside
  inout_report <- print("Only indoor sensor data will be loaded.")
}
if(include_outside == FALSE & include_inside == FALSE){
  inout_report <- warning("INPUT ERROR: Please set `include_outside` and/or `include_inside` to TRUE.")
  stop(inout_report)
}

# Message with feedback as to which monitors will be selected
inout_report

# Warning message if there are many monitors
if(length(ids) > 100){
  warning("CAUTION: More than 100 monitors selected. Data processing will take significant time. Consider applying more area filters first!")
}
# Message returning the number of monitors that will be selected
print(paste("Number of selected monitors:", length(ids)))

# Cleaning environment
remove(ids_outside, ids_inside, inout_report)
```

```{r, getting_pat}
## Setup for iteration
# URL to be used to grab data
input_baseUrl <- "https://api.thingspeak.com/channels/"
# Number of IDs to be evaluated. Used for console messages.
id_count <- length(ids)
# Starting the counts at 0; will grow upon iteration
count <- 0 
# Creating empty objects to be filled upon iteration appends/row-binds
pat_single <- list()
raw_meta <- data.frame()
data_a <- data.frame()
data_b <- data.frame()
# Columns of interest
input_cols_to_select <- c("created_at", "temperature", "humidity", "pm2.5_cf1", "pm2.5_atm")

# Sequencing dates into a list by 1 week gaps due to API limitations
date_sequence <- seq(from = as.Date(input_startdate), to = as.Date(input_enddate), by = "week")
# Adding end date to the end of the sequence if not already present in the set
if(tail(date_sequence, n=1) != input_enddate){
  date_sequence <- date_sequence %>% 
    append(input_enddate)
}

# Iteration
for (id in ids[1:id_count]) {
  
  # Message to report progress
  count <- count + 1
  print(sprintf("Working on %s (%d/%d) ...", id, count, id_count))
  print(sprintf("... from %s to %s", date_sequence[1], date_sequence[2]))
  
  # Creating single PAT for first date range
  pat_single <- pat_downloadParseRawData(
    id = id,
    label = NULL,
    pas = pas_area,
    startdate = date_sequence[1],
    enddate = date_sequence[2],
    baseUrl = input_baseUrl
  )
  
  data_a_single <- pat_single$A_PRIMARY %>% 
    select(intersect(input_cols_to_select, names(.))) %>% 
    mutate("site_id" = id)
  
  data_b_single <- pat_single$B_PRIMARY %>%
    select(intersect(input_cols_to_select, names(.))) %>% 
    mutate("site_id" = id)
  
  # If there is more than a single pair of dates in the date sequence, sub-iterations will occur
  if (length(date_sequence) > 2) {
    
    for (i in 2:(length(date_sequence)-1) ) {
      
      print(sprintf("... from %s to %s", date_sequence[i], date_sequence[i+1]))
      
      pat_single_more <- pat_downloadParseRawData(
        id = id,
        label = NULL,
        pas = pas_area,
        startdate = date_sequence[i],
        enddate = date_sequence[i + 1],
        baseUrl = input_baseUrl
      )
      
      data_a_more <- pat_single_more$A_PRIMARY %>% 
        select(intersect(input_cols_to_select, names(.))) %>% 
        mutate("site_id" = id)
      
      data_b_more <- pat_single_more$B_PRIMARY %>% 
        select(intersect(input_cols_to_select, names(.))) %>% 
        mutate("site_id" = id)
      
      data_a_single <- rbind(data_a_single, data_a_more)
      data_b_single <- rbind(data_b_single, data_b_more)
      
      # Cleaning environment
      remove(pat_single_more, data_a_more, data_b_more, i)
    }
  }
  
  raw_meta_single <- pat_single$meta %>% 
    drop_na(DEVICE_LOCATIONTYPE) %>% 
    mutate("site_id" = id)
  
  raw_meta <- rbind(raw_meta, raw_meta_single)
  data_a <- rbind(data_a, data_a_single)
  data_b <- rbind(data_b, data_b_single)
  
  # Cleaning environment
  remove(data_a_single, data_b_single, pat_single, raw_meta_single)
}

# Joining data frames
raw_data <- full_join(data_a, data_b, # Data sets to join
                      by = c("created_at", "site_id"), # Columns to unite
                      suffix = c("_A", "_B")) %>% # Adding custom suffixes to differentiate the columns
  # Rounding for low time increments, to allow for mostly parallel A & B reports
  mutate(datetime = floor_date(created_at, unit = "2 minutes")) %>% 
  # Grouping for the summarization that will follow
  group_by(site_id, datetime) %>% 
  # Summarizing all numeric columns. Removing NAs prevents values being removed for having an NA in the group
  summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
  # Removing periods from column headers
  rename_with(~gsub("\\.", "", .x))

# Review
print(sprintf("Successfully created %d/%d pat objects.", length(unique(raw_data$site_id)), id_count))
```

```{r, eclean_iteration}
# Already have both saved in raw_data
remove(data_a, data_b)
```

```{r, function_tz}
# Time zones are reported as a variable when downloading the data. Time stamps are reported in UTC. The following will convert the time stamps to said reported time zone.
# If more than one time zone is reported in the data, the conversion will use the time zone most frequently used in the data set. This is because no more than one time zone can be applied to a date time variable.
adjust_timezone <- function(dataset, location_data = raw_meta){
  
  # Translation: If more than 1 unique timezone is reported in the data frame, then:
  if(length(unique(location_data$timezone)) > 1){
  print("Multiple time zones reported. Timestamp will be based on the most frequent time zone reported.")
  timezone <- (location_data %>% 
                 group_by(timezone) %>% 
                 count() %>% 
                 arrange(desc(n)) %>% 
                 pull(timezone))[1]
  } else {
    timezone <- unique(location_data$timezone)
  }
  
  dataset_new <- dataset %>%
    # Original time stamps (in UTC) will be preserved
    rename(datetime_utc = datetime) %>% 
    # Creating variables for date and hour
    mutate(
      datetime = with_tz(datetime_utc, tzone = timezone),
      date = date(datetime),
      date_hour = floor_date(datetime, unit = "hour")
    )
  
  print(paste("Time zone applied:", timezone))
  return(dataset_new)
}
```

```{r, function_qc}
apply_qc <- function(dataset, avg_ab = TRUE){
  
  # Creating columns to average A & B data
  if(avg_ab == TRUE){
    dataset <- dataset %>% 
      rowwise() %>% 
      mutate(
        pm25_cf1 = mean(c(pm25_cf1_A, pm25_cf1_B), na.rm = TRUE),
        pm25_atm = mean(c(pm25_atm_A, pm25_atm_B), na.rm = TRUE)
      ) %>% 
      ungroup()
    print("Columns for averages of A & B data added")
  }
  
  data_qc <- dataset %>% 
    # Basic quality control; only physically possible values (and real numbers) are kept
    filter_at(
      # Selecting the columns that start with 'pm25'
      vars(starts_with("pm25")),
      # Filtering said columns such that only values 0:2000 are kept
      all_vars(between(., 0, 2000))
    ) %>% 
    filter(
      # Filtering temperature for -40:185
      between(temperature, -40, 185),
      # Filtering humidity for 0:100
      between(humidity, 0, 100)
    )
  
  return(data_qc)
}
```

```{r, data_qc}
# Basic subsetting and quality control

data_meta <- raw_meta %>% 
  # Selecting only variables of interest, to save on storage
  select(site_id, DEVICE_LOCATIONTYPE, label, longitude, latitude, timezone)

data_pm25 <- raw_data %>% 
  # Applying quality control (see above function)
  apply_qc() %>% 
  # Adjusting to be reported time zone (see above function)
  adjust_timezone() %>% 
  # Reordering variables for ease of reading
  select(site_id, datetime, date, date_hour, everything())
```

```{r, eclean_qc, eval=FALSE}
remove(apply_qc, adjust_timezone)
```

```{r, function_epa}
apply_epa <- function(dataset, by_day = TRUE, by_hour = FALSE, epa_percent = 75){
  
  if(by_day == FALSE & by_hour == FALSE){
    stop("INPUT ERROR: Please set `by_day` and/or `by_hour` to TRUE.")
  }
  if(by_day == TRUE & by_hour == FALSE){
    print("Grouping by date (24 hour averages, by day) [default]")
    groupings_drop <- vars(site_id, date, date_hour)
    groupings <- vars(site_id, date)
    time_unit <- 24
  }
  if(by_day == TRUE & by_hour == TRUE){
    print("Grouping by date and hour (1 hour averages, by day)")
    dataset <- dataset %>% 
      mutate(minute = minute(datetime))
    groupings_drop <- vars(site_id, date, date_hour, minute)
    groupings <- vars(site_id, date, date_hour)
    time_unit <- 60/2
  }
  if(by_day == FALSE & by_hour == TRUE){
    print("Grouping by hour (1 hour averages)")
    dataset <- dataset %>% 
      mutate(
        hour = hour(datetime),
        minute = minute(datetime)
      )
    groupings_drop <- vars(site_id, hour, minute)
    groupings <- vars(site_id, hour)
    time_unit <- 60/2
  }
  
  # Calculating the minimum number of data points required to be included in the set
  count_to_drop <- ceiling(time_unit*(epa_percent/100))
  
  # Creating a data frame of groups to be removed due to low data quantity
  drop_quantity <- dataset %>% 
    group_by_at(groupings_drop) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    group_by_at(groupings) %>% 
    count() %>% 
    filter(n < count_to_drop)
  
  print("Values to be dropped via an anti-join due to low data quantity:")
  print(drop_quantity)
  
  drop_ab <- dataset %>% 
    group_by(site_id, date) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    mutate(
      # Difference between A & B sensors
      diff = pm25_cf1_A-pm25_cf1_B,
      # Percentage difference between A & B sensors
      per_diff = 100*(abs(pm25_cf1_A-pm25_cf1_B))/((pm25_cf1_A+pm25_cf1_B)/2),
      drop = case_when(
        # Will drop the following based on EPA recommendations
        abs(diff) >= 5 & abs(per_diff) >= 62 ~ TRUE,
        # Fills all other values (the ones to be kept) with 'false'
        TRUE ~ FALSE
        )
      ) %>% 
    filter(drop == TRUE) %>% 
    select(site_id, date)
  
  print("Days (by sensor) to be dropped via an anti-join due to A & B sensor disagreement:")
  print(drop_ab)
  
  dataset %>% 
    anti_join(drop_quantity) %>% 
    anti_join(drop_ab) %>% 
    group_by_at(groupings) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      # US-wide correction factor published in 2020
      epa_2020 = 0.524*pm25_cf1 - 0.0852*humidity + 5.72,
      # US-wide correction factor published in late 2020
      epa_2021 = case_when(
        pm25_cf1 <= 343 ~ 0.52*pm25_cf1 - 0.086*humidity + 5.75,
        pm25_cf1 > 343 ~ 0.46*pm25_cf1 + (3.93*10^(-4))*(pm25_cf1^2) +2.97
      ),
      # US-wide correction factor published in late 2020 using CF=ATM values
      epa_atm = case_when(
        pm25_atm < 50 ~ 0.25*pm25_atm - 0.086*humidity + 5.75,
        pm25_atm >= 50 & pm25_atm < 229 ~ 0.786*pm25_atm - 0.086*humidity + 5.75,
        pm25_atm > 229 ~ 0.69*pm25_atm + (8.84*10^-4)*(pm25_atm^2) + 2.97
      )
    ) %>% 
    # Setting negative values to NA
    mutate_at(vars(epa_2020, epa_2021, epa_atm), ~replace(., which(.<0), NA))
    # # Dropping negative values
    # filter_at(vars(epa_2020, epa_2021, epa_atm), all_vars(. >= 0))
}
```

```{r, function_lrapa}
apply_lrapa <- function(dataset, by_day = TRUE, by_hour = FALSE){
  
  if(by_day == FALSE & by_hour == FALSE){
    stop("INPUT ERROR: Please set `by_day` and/or `by_hour` to TRUE.")
  }
  if(by_day == TRUE & by_hour == FALSE){
    print("Grouping by date (24 hour averages, by day) [DEFAULT]")
    groupings <- vars(site_id, date)
  }
  if(by_day == TRUE & by_hour == TRUE){
    print("Grouping by date and hour (1 hour averages, by day)")
    groupings <- vars(site_id, date, date_hour)
  }
  if(by_day == FALSE & by_hour == TRUE){
    print("Grouping by hour (1 hour averages)")
    groupings <- vars(site_id, hour)
    dataset <- dataset %>% 
      mutate(hour = hour(datetime))
  }
  
  dataset %>% 
    group_by_at(groupings) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      lrapa = case_when(pm25_cf1 <= 65 ~ 0.5 * pm25_atm - 0.66)
      ) %>% 
    # Setting negative values to NA
    mutate_at(vars(lrapa), ~replace(., which(.<0), NA))
    # # Dropping negative values
    # filter(lrapa >= 0)
}
```

```{r, function_corrections}
apply_corrections <- function(data, daily, hourly, epa = 75){
  epa <- data %>% 
    apply_epa(by_day = daily, by_hour = hourly, epa_percent = epa)
  print("EPA corrections applied")
  
  lrapa <- data %>% 
    apply_lrapa(by_day = daily, by_hour = hourly)
  print("LRAPA corrections applied")
  
  df_corr <- epa %>% 
    full_join(lrapa)
  print("Data frame of corrected values created")
  return(df_corr)
}
```

```{r, function_tag_dates}
apply_date_tags <- function(dataset,
                        starts = input_date_starts, ends = input_date_ends, tags = input_date_tags,
                        date_stamp = "17 Jan 1999"){
  
  starts <- as.Date(starts)
  ends <- as.Date(ends)
  tags <- factor(tags)
  date_format <- stamp_date(date_stamp)
  
  data_temp <- data.frame(starts, ends, tags) %>% 
    arrange(starts) %>% 
    mutate(
      start_stamp = date_format(starts),
      end_stamp = date_format(ends),
      date_tag = fct_inorder(paste0(tags, "\n(", start_stamp, " - ", end_stamp, ")"))
    ) %>% 
    pivot_longer(cols = c(starts, ends), values_to = "date") %>% 
    group_by(date_tag) %>% 
    complete(date = full_seq(date, 1)) %>% 
    select(!c(name, tags, start_stamp, end_stamp))
  
  dataset %>% 
    left_join(data_temp) %>% 
    drop_na(date_tag)
}
```

```{r, function_tag_hours}
apply_hour_tags <- function(dataset, starts = input_hour_starts, tags = input_hour_tags){
  tags <- factor(tags)
  
  hours_in_day <- data.frame(starts = 0:23)
  
  data_temp <- data.frame(starts, tags) %>% 
    mutate(
      ends = lead(starts - 1),
      ends = replace_na(ends, starts[1]-1),
      tags = paste0(tags, "\n(", formatC(starts, width=2, flag=0), ":00-", formatC(ends, width=2, flag=0), ":59)")
    ) %>% 
    arrange(starts) %>% 
    mutate(hour_tag = fct_inorder(tags)) %>%
    complete(starts = full_seq(starts, 1)) %>%
    right_join(hours_in_day) %>%
    rename(hour = starts) %>%
    fill(hour_tag) %>%
    select(!c(tags, ends))
  
  if("hour" %in% colnames(dataset) == FALSE){
    dataset <- dataset %>% 
      mutate(hour = hour(date_hour))
  }
  
  dataset %>% 
    left_join(data_temp)
}
```

```{r, function_applying}
applying_functions <- function(dataframe, group_day, group_hour, tag_dates = run_date_grouping, tag_hours = run_hour_grouping){
  
  # Applying correction factors
  new_df <- dataframe %>%
    apply_corrections(daily = group_day, hourly = group_hour)
  print("Correction factors applied.")
  
  # Tagging by date
  if(tag_dates == TRUE & group_day == TRUE){
    # tagged_days <- dataframe %>% 
    #   apply_date_tags() %>% 
    #   select(date, date_tag)
    tagged_days <- dataframe %>% 
      select(date) %>% 
      distinct() %>% 
      apply_date_tags() %>% 
      select(date, date_tag)
    
    new_df <- new_df %>% left_join(tagged_days)
    print("Data now tagged by provided date groupings.")
  } else {
    print("Date groups not applied.")
  }
  
  # Tagging by hour
  if(group_hour == TRUE & tag_hours == TRUE){
    # tagged_hours <- dataframe %>% 
    #   apply_hour_tags() %>% 
    #   select(hour, hour_tag)
    tagged_hours <- dataframe %>% 
      select(date_hour) %>% 
      distinct() %>% 
      apply_hour_tags() %>% 
      select(hour, hour_tag) %>% 
      distinct()
    
    if("hour" %in% colnames(new_df) == FALSE){
      new_df <- new_df %>% 
        mutate(hour = hour(date_hour))
    }
    
    new_df <- new_df %>% left_join(tagged_hours)
    print("Data now tagged by provided hour groupings.")
    
  } else {
    print("Hour groups not applied.")
  }
  
  # Returning the adjusted dataframe
  return(new_df)
}
```

```{r, data_applying}
data_hourly <- applying_functions(data_pm25, group_day = TRUE, group_hour = TRUE)
data_daily <- applying_functions(data_pm25, group_day = TRUE, group_hour = FALSE)
data_diurnal <- applying_functions(data_pm25, group_day = FALSE, group_hour = TRUE)
```

```{r, eclean_applying, eval=FALSE}
remove(apply_corrections, applying_functions, apply_epa, apply_lrapa, apply_hour_tags, apply_date_tags)
```

```{r, function_map}
# FUNCTION
# Required inputs: pm_data (df), value (column of PM2.5 data w/in df to map), location_data (lat/long data to be joined to df; default: data_meta)
# Optional inputs: grouping variables (1 or 2) w/ respective custom exclusions; filter for indoor/outdoor data
# Other customization for map: type (stamen), zoom, tint & color, point size, color scale (virids)

map_q <- function(pm_data, value, location_data = data_meta,
                  grouping_var = NULL, exclude = NULL,
                  grouping_var_2 = NULL, exclude_2 = NULL,
                  outside = include_outside, inside = include_inside,
                  label = c(""),
                  maptype = "toner-lite", zoom = 11, tint = 0.5, tint_color = "black",
                  point_size = 3, viridis = "viridis"){
  
  # Warning message and halting execution if required input is missing.
  if(missing(value) == TRUE) stop ("ERROR in argument 'value': Missing input, with no default. Execution haulted. Please input a valid PM2.5 variable (e.g. epa_2021).")
  
  # Warning message and halting execution if inputeded data does not have more than one unique site_id (map will bug)
  if(length(unique(pm_data$site_id)) == 1) stop("ERROR: Insufficient data to create a map. Provide a data set with more than one site.")
  
  input_labels <- paste0("(", paste(label, collapse = ")|("), ")")
  
  # Wrangling based on provided inputs
  data_temp <- pm_data %>% 
    ungroup() %>% 
    # Grouping by provided variables, as well as site_id
    group_by_at(vars(site_id, {{grouping_var}}, {{grouping_var_2}})) %>% 
    # Calculating means by said grouping variables
    summarize(mean = mean({{value}}, na.rm = TRUE)) %>% 
    drop_na(mean) %>% 
    # Adding location data to get lat & lon
    left_join(location_data) %>% 
    # Selecting desired labels, if provided
    filter(str_detect(label, input_labels))
  
  # Feedback message
  print("Data now grouped and averaged. Location data added.")
  
  # Custom filtering: exclusions for input categories
  if(is.null(exclude) == FALSE){
    to_exclude <- paste(exclude, collapse = "|")
    data_temp <- data_temp %>% 
      filter(!str_detect({{grouping_var}}, to_exclude))
    print(paste("Excluded", exclude, "from", deparse(substitute(grouping_var))))
  }
  if(is.null(exclude_2) == FALSE){
    to_exclude_2 <- paste(exclude_2, collapse = "|")
    data_temp <- data_temp %>% 
      filter(!str_detect({{grouping_var_2}}, to_exclude_2))
    print(paste("Excluded", exclude_2, "from", deparse(substitute(grouping_var_2))))
  }
  
  # Default shapes for inside/outside
  shapes_inout <- c("outside" = 21, "inside" = 23)
  
  # Custom filtering: exclusions for inside/outside
  # Will also override shape arguments to prevent excluded category from showing in the legend
  if(outside == FALSE){
    data_temp <- data_temp %>% 
      filter(DEVICE_LOCATIONTYPE != "outside")
    print("Outdoor data excluded")
    shapes_inout <- c("inside" = 23)
  }
  if(inside == FALSE){
    data_temp <- data_temp %>% 
      filter(DEVICE_LOCATIONTYPE != "inside")
    print("Indoor data excluded")
    shapes_inout <- c("outside" = 21)
  }
  
  
  # Labels for the plot
  lab_title <- expression("PM"[2.5]*" values")
  lab_subtitle <- paste("Data from PurpleAir. Data plotted:", deparse(substitute(value)))
  lab_pm <- expression(paste("Average PM"[2.5]*" (", mu, "g/m"^3*"):"))
  
  # Base plot
  plot <- qmplot(data = data_temp, x = longitude, y = latitude,
         geom = "blank", maptype = maptype, zoom = zoom, darken = c(tint, tint_color)) +
    facet_wrap(vars({{grouping_var}})) +
    geom_point(
      aes(fill = mean, shape = DEVICE_LOCATIONTYPE),
      color = "white",
      alpha = 0.8,
      size = point_size
    ) +
    scale_fill_viridis(
      option = {{viridis}},
      direction = -1,
      end = 0.85,
      limits = c(0, NA)
    ) +
    theme_void() +
    scale_shape_manual(values = shapes_inout) +
    theme(
      legend.position = "bottom",
      legend.box = "vertical"
    ) + 
    guides(shape = guide_legend(override.aes = list(fill="black"))) +
    labs(
      title = lab_title,
      subtitle = lab_subtitle,
      fill = lab_pm,
      shape = "Monitor location:",
      caption = "Data not grouped."
    )
  
  # Feedback message
  print("Base plot created.")
  
  ## Faceting by grouping variable(s) and applying an appropriate caption to reflect the grouping
  # Facet wrap if 1 grouping variable provided
  if(deparse(substitute(grouping_var)) != "NULL" &
     deparse(substitute(grouping_var_2)) == "NULL"
  ){
    plot <- plot +
      facet_wrap(vars({{grouping_var}})) +
      labs(
        caption = paste(
          "Data grouped by",
          (colnames(pm_data %>% select({{grouping_var}})))[1]
        )
      )
    # Feedback message
    print(paste("Plot now faceted by", deparse(substitute(grouping_var))))
  }
  # Facet grid if 2 grouping variables provided
  if(deparse(substitute(grouping_var)) != "NULL" &
     deparse(substitute(grouping_var_2)) != "NULL"
  ){
    plot <- plot +
      facet_grid(
      formula(paste(
        vars({{grouping_var_2}}),
        "~",
        vars({{grouping_var}})
      ))) +
      labs(
        caption = paste(
          "Data grouped by",
          deparse(substitute(grouping_var)),
          "and",
          deparse(substitute(grouping_var_2))
          )
      )
    # Feedback message
    print(paste("Plot now faceted by",
                deparse(substitute(grouping_var)),
                "and",
                deparse(substitute(grouping_var_2))
                ))
  }
  
  print("Final plot created.")
  
  # Returning the final plot
  plot
}
```

```{r, viz_maps, eval=FALSE}
map_q(data_hourly, value = epa_2021, grouping_var = hour_tag)

data_hourly %>% 
  filter(site_id != "9c83953c1afae9df_86135") %>% 
  map_q(value = epa_2021, grouping_var = hour, grouping_var_2 = date_tag, inside=FALSE)

data_hourly %>% filter(site_id != "9c83953c1afae9df_86135") %>% 
  map_q(value=epa_2021, grouping_var = date_tag, inside=FALSE)

data_hourly %>% 
  # filter(site_id != "01cff4b431a1862a_26757",
  #        site_id != "65b3dca6d412ed31_55407") %>% 
  map_q(value = epa_2021, grouping_var_2 = hour_tag, grouping_var = date_tag, exclude = c("Before", "Fire"))

map_q(data_daily, value = epa_2020, grouping_var = date_tag, inside = FALSE)

map_q(data_daily, value = lrapa, grouping_var = date_tag, exclude = "Fire")

map_q(data_hourly, value = pm25_atm, grouping_var = hour_tag, exclude = "Afternoon", point_size = 5,
      maptype = "terrain", tint = 0.3, tint_color = "white", viridis = "magma")
```

```{r, viz_time, eval=FALSE}
long_set <- data_hourly %>% 
  pivot_longer(cols = c(pm25_cf1, epa_2020, epa_2021, epa_atm, lrapa, pm25_atm),
               names_to = "corr",
               values_to = "pm25") %>% 
  mutate(
    corr = fct_inorder(corr),
    source = fct_collapse(corr,
      PA = c("pm25_cf1", "pm25_atm"),
      EPA = c("epa_2020", "epa_2021", "epa_atm"),
      LRAPA = "lrapa"
    )
  )

long_set %>% 
  ggplot(aes(date_hour, pm25, color = corr)) +
  facet_wrap(~date_tag, scales = "free") +
  geom_point(alpha = 0.1, shape = 1) +
  stat_smooth() +
  #geom_line()
  theme(legend.position = "bottom")

long_set %>% 
  #ggplot(aes(source, value, color = name)) +
  ggplot(aes(corr, pm25, color = source)) +
  #facet_wrap(~date_tag, scales = "free") +
  facet_wrap(~hour_tag, scales = "free") +
  #facet_wrap(~date_tag) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), linetype = "dotted") +
  geom_violin(draw_quantiles = 0.5, fill = "transparent") +
  stat_summary(fun.y = mean, geom ="point", color = "red") +
  theme(legend.position = "bottom")

long_set %>% 
  select(!c(pm25_cf1_A,pm25_cf1_B,pm25_atm_A,pm25_atm_B)) %>% 
  mutate(wday = lubridate::wday(date,label=TRUE)) %>% 
  #group_by(site_id, hour, wday, corr, source) %>% 
  group_by(hour, wday, corr, source) %>% 
  summarize_if(is.numeric, mean, na.rm=TRUE) %>% 
  ggplot(aes(x=hour, y=pm25, color=corr)) +
  facet_grid(source~wday, scales="free_y") +
  geom_line() +
  theme(legend.position="bottom")
```
