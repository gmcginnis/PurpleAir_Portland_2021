---
title: "summer_api_scripting"
author: "Gillian McGinnis"
date: "6/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r fixing_qmplot, eval=FALSE}
detach("package:ggmap")
remove.packages("ggmap")

devtools::install_github("dkahle/ggmap")

.rs.restartR()
library(ggmap)
```


```{r libraries}
library(tidyverse)
library(AirSensor)
library(lubridate)
library(ggmap)
library(viridis)
library(ggrepel)
```

```{r inputs}
# REQUIRED: Start and end dates of interest. Please use the following format: "YYYY-MM-DD", with leading zeros where appropriate
# Data will be pulled from the start of the start date through the end of the end date
# input_startdate <- "2021-07-01"
# input_enddate <- "2021-07-08"
input_startdate <- "2021-07-10"
input_enddate <- "2021-07-13"

## REQUIRED: Inside/Outside argument. At least one of the following must be TRUE.
# Grab data for outdoor sensors?
include_outside <- TRUE
# Grab data for indoor sensors?
include_inside <- TRUE

# Drop monitors flagged as reporting high values?
input_drop_hi <- TRUE

## INPUTS FOR GETTING PAS & PAT
## The following are optional but recommended. Set to NULL if not interested in filtering by the following.
# State of monitors to include. If NULL/unspecified, all monitors specified in the above boundaries will be included regardless of state
input_stateCode <- "OR"

# Boundaries (in degrees) of monitors to include. It can also evaluate n/s or e/w pairs of bounds individually.
# Helpful website: bboxfinder [dot] com
input_west <- -122.854
input_south <- 45.4
input_east <- -122.58
input_north <- 45.6

# Labels of monitors to include. If NULL/unspecified, all monitors will be included.
# The labels are based on string detection, so for instance having "STAR" will pull all that have the word "STAR" in the label.
# Example format below. Capitalization matters!
# \\b is added where we want only values that report those words on their own (ex. STAR-LAB and not STARSYSTEM1)
# input_labels <- c("se", "SE", "Se", "\\bSTAR\\b", "\\bPSU\\b")
input_labels <- c("Indigo")

# Optional grouping settings
## DATE GROUPING: group values into tags (will NOT impact raw data output, but can be useful when graphing)
# Run date grouping? If set to "FALSE", the date inputs below will not matter (but will still appear in the environment)
run_date_grouping <- FALSE
# Date grouping categories:
input_date_tags <- c("Before", "Independence Day", "After")
# Start dates (in a list, "YYYY-MM-DD")
input_date_starts <- c("2021-07-01", "2021-07-04", "2021-07-05")
# End dates (in a list, "YYYY-MM-DD")
input_date_ends <- c("2021-07-03", "2021-07-04", "2021-07-08")

## HOUR GROUPING: group values into tags (will NOT impact raw data output, but can be useful when graphing)
# Run hour grouping? If set to "FALSE", the hour inputs below will not matter (but will still appear in the environment)
run_hour_grouping <- TRUE
# Hour grouping categories:
input_hour_tags <- c("Morning", "Afternoon", "Evening", "Night")
# Start hours (in a list, using full hours in 24 hour format)
input_hour_starts <- c(5, 12, 17, 21)
# End hours will be automatically generated
```



```{r getting_pas}
get_area_pas <- function(state_code = input_stateCode,
                         west = input_west, east = input_east, south = input_south, north = input_north,
                         labels = input_labels,
                         datestamp = input_enddate, startdate = input_startdate){
  
  setArchiveBaseUrl("http://data.mazamascience.com/PurpleAir/v1")
  
  # Stringing the selected labels as one argument to be used as a string
  labels_string <- paste0("(", paste(labels, collapse = ")|("), ")")
  
  lookback_days <- as.numeric(as.Date(datestamp) - as.Date(startdate)) + 1
  
  pas <- pas_load(
    # archival allows for retrieval of sensor info even if they are no longer reporting
    archival = TRUE,
    # datestamp is important when loading historical data, as not all monitors might be actively reporting anymore
    datestamp = str_replace_all(as.character(datestamp), "-", ""),
    # Retries (default: 30 days) are the maximum number of days to go back and try to load data if the requested date cannot be retrieved
    retries = lookback_days
  )
  
  # Filtering (or not) filtering by state
  if (is.null(state_code) == TRUE) {
    pas_state <- pas
    print("Sensors not filtered for a specified state code.")
  } else {
    pas_state <- pas_filter(pas, stateCode == state_code)
    print(paste("Sensors now filtered for the state of", state_code))
  }
  
  pas_area <- pas_filterArea(pas = pas_state, w = west, e = east, s = south, n = north) %>% 
    pas_filter(str_detect(label, labels_string))
  
  return(pas_area)
}

pas_area <- get_area_pas()
```

```{r eclean_pas}
# Cleaning environment
remove(get_area_pas, input_stateCode, input_north, input_east, input_south, input_west, input_labels)
```

```{r getting_ids}
ids_outside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = TRUE)
ids_inside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = FALSE)

if (include_outside == TRUE & include_inside == TRUE) {
  ids <- c(ids_outside, ids_inside)
  inout_report <- print("Both indoor and outdoor sensor data will be loaded.")
}
if (include_outside == TRUE & include_inside == FALSE) {
  ids <- ids_outside
  inout_report <- print("Only outdoor sensor data will be loaded.")
}
if (include_outside == FALSE & include_inside == TRUE) {
  ids <- ids_inside
  inout_report <- print("Only indoor sensor data will be loaded.")
}
if (include_outside == FALSE & include_inside == FALSE) {
  inout_report <- warning("INPUT ERROR: Please set `include_outside` and/or `include_inside` to TRUE.")
  stop(inout_report)
}

# Message with feedback as to which monitors will be selected
inout_report

# Warning message if there are many monitors
if (length(ids) > 100) {
  warning("CAUTION: More than 100 monitors selected. Data processing will take significant time. Consider applying more area filters first!")
}
# Message returning the number of monitors that will be selected
print(paste("Number of selected monitors:", length(ids)))
```

```{r eclean_ids}
remove(ids_outside, ids_inside, inout_report)
```

```{r getting_pat}
## Setup for iteration
# URL to be used to grab data
input_baseUrl <- "https://api.thingspeak.com/channels/"
# Number of IDs to be evaluated. Used for console messages.
id_count <- length(ids)
# Starting the counts at 0; will grow upon iteration
count <- 0 
# Creating empty objects to be filled upon iteration appends/row-binds
pat_single <- list()
raw_meta <- data.frame()
data_a <- data.frame()
data_b <- data.frame()
# Columns of interest
input_cols_to_select <- c("created_at", "temperature", "humidity", "pm2.5_cf1", "pm2.5_atm")

input_enddate_1 <- as.Date(input_enddate) + 1
# Sequencing dates into a list by 1 week gaps due to API limitations
date_sequence <- seq(from = as.Date(input_startdate), to = input_enddate_1, by = "week")
# Adding end date to the end of the sequence if not already present in the set
if (tail(date_sequence, n=1) != input_enddate_1) {
  date_sequence <- date_sequence %>%
    append(input_enddate_1)
}
remove(input_enddate_1)

# Iteration
for (id in ids[1:id_count]) {
  
  # Message to report progress
  count <- count + 1
  print(sprintf("Working on %s (%d/%d) ...", id, count, id_count))
  print(sprintf("... from %s to %s", date_sequence[1], date_sequence[2]))
  
  # Creating single PAT for first date range
  pat_single <- pat_downloadParseRawData(
    id = id,
    label = NULL,
    pas = pas_area,
    startdate = date_sequence[1],
    enddate = date_sequence[2],
    baseUrl = input_baseUrl
  )
  
  data_a_single <- pat_single$A_PRIMARY %>% 
    select(intersect(input_cols_to_select, names(.))) %>% 
    mutate("site_id" = id)
  
  data_b_single <- pat_single$B_PRIMARY %>%
    select(intersect(input_cols_to_select, names(.))) %>% 
    mutate("site_id" = id)
  
  # If there is more than a single pair of dates in the date sequence, sub-iterations will occur
  if (length(date_sequence) > 2) {
    
    for (single_position in 2:(length(date_sequence)-1) ) {
      
      print(sprintf("... from %s to %s", date_sequence[single_position], date_sequence[single_position+1]))
      
      pat_single_more <- pat_downloadParseRawData(
        id = id,
        label = NULL,
        pas = pas_area,
        startdate = date_sequence[single_position],
        enddate = date_sequence[single_position + 1],
        baseUrl = input_baseUrl
      )
      
      data_a_more <- pat_single_more$A_PRIMARY %>% 
        select(intersect(input_cols_to_select, names(.))) %>% 
        mutate("site_id" = id)
      
      data_b_more <- pat_single_more$B_PRIMARY %>% 
        select(intersect(input_cols_to_select, names(.))) %>% 
        mutate("site_id" = id)
      
      data_a_single <- rbind(data_a_single, data_a_more)
      data_b_single <- rbind(data_b_single, data_b_more)
      
      # Cleaning environment
      remove(pat_single_more, data_a_more, data_b_more, single_position)
    }
  }
  
  raw_meta_single <- pat_single$meta %>% 
    drop_na(DEVICE_LOCATIONTYPE) %>% 
    mutate("site_id" = id)
  
  raw_meta <- rbind(raw_meta, raw_meta_single)
  data_a <- rbind(data_a, data_a_single)
  data_b <- rbind(data_b, data_b_single)
  
  # Cleaning environment
  remove(data_a_single, data_b_single, pat_single, raw_meta_single)
}
```

```{r eclean_iteration}
remove(count, id, ids, input_baseUrl, input_cols_to_select, date_sequence)
```

```{r pat_joining}
# Joining data frames
raw_data <- full_join(data_a, data_b, # Data sets to join
                      by = c("created_at", "site_id"), # Columns to unite
                      suffix = c("_A", "_B")) %>% # Adding custom suffixes to differentiate the columns
  # Rounding for low time increments, to allow for mostly parallel A & B reports
  mutate(datetime = floor_date(created_at, unit = "2 minutes")) %>% 
  # Grouping for the summarization that will follow
  group_by(site_id, datetime) %>% 
  # Summarizing all numeric columns. Removing NAs prevents values being removed for having an NA in the group
  summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
  # Removing periods from column headers
  rename_with(~gsub("\\.", "", .x))

# Review
print(sprintf("Successfully created %d/%d pat objects.", length(unique(raw_data$site_id)), id_count))
remove(id_count)
```

```{r eclean_joining}
# Already have both saved in raw_data
remove(data_a, data_b)
```

```{r function_tz}
# Time zones are reported as a variable when downloading the data. Time stamps are reported in UTC. The following will convert the time stamps to said reported time zone.
# If more than one time zone is reported in the data, the conversion will use the time zone most frequently used in the data set. This is because no more than one time zone can be applied to a date time variable.
adjust_timezone <- function(dataset, location_data = raw_meta){
  
  # Translation: If more than 1 unique timezone is reported in the data frame, then:
  if (length(unique(location_data$timezone)) > 1) {
  print("Multiple time zones reported. Timestamp will be based on the most frequent time zone reported.")
  timezone <- (location_data %>% 
                 group_by(timezone) %>% 
                 count() %>% 
                 arrange(desc(n)) %>% 
                 pull(timezone))[1]
  } else {
    timezone <- unique(location_data$timezone)
  }
  
  dataset <- dataset %>%
    # Original time stamps (in UTC) will be preserved
    rename(datetime_utc = datetime) %>% 
    mutate(datetime = with_tz(datetime_utc, tzone = timezone))
  
  print(paste("Time zone applied:", timezone))
  return(dataset)
}
```

```{r function_dt}
column_dt <- function(dataset, unit){
  
  if (
    "datetime" %in% colnames(dataset) == FALSE &
    "date_hour" %in% colnames(dataset) == FALSE
    ) { stop("ERROR: columns `datetime` and `date_hour` not found in dataset. Execusion halted.") }
  
  for (unit_single in unit) {
    if (unit_single %in% c("date", "date_hour", "hour", "hour_minute", "time") == FALSE) {
      stop(paste0("Unrecognized unit: \`", unit_single, "\`. Please use `date`, `date_hour`, `hour`, `hour_minute`, or `time`."))
    }
  }
  
  if ("date" %in% unit) {
    if ("date_hour" %in% colnames(dataset) == TRUE) {
      dataset <- dataset %>% 
        mutate(date = date(date_hour))
      print("`date` column added from `date_hour`")
    } else {
      dataset <- dataset %>%
        mutate(date = date(datetime))
      print("`date` column added")
    }
  }
  
  if ("date_hour" %in% unit) {
    dataset <- dataset %>% 
      mutate(date_hour = floor_date(datetime, unit = "hour"))
    print("`date_hour` column added")
  }
  
  if ("time" %in% unit) {
    dataset <- dataset %>% 
      mutate(time = hms::as_hms(datetime))
    print("`time` column added")
  }
  
  if ("hour" %in% unit) {
    if ("date_hour" %in% colnames(dataset) == TRUE) {
      dataset <- dataset %>% 
        mutate(hour = hms::as_hms(date_hour))
      print("`hour` column added")
    } else {
      dataset <- dataset %>% 
        mutate(
          date_hour = floor_date(datetime, unit = "hour"),
          hour = hms::as_hms(date_hour)
        ) %>% 
        select(!date_hour)
      print("`hour` column added, `date_hour` column created then disgarded")
    }
  }
  
  if ("hour_minute" %in% unit) {
    dataset <- dataset %>%
      mutate(
        date_minute = floor_date(datetime, unit = "minute"),
        hour_minute = hms::as_hms(date_minute)
      ) %>%
      select(!date_minute)
    print("`hour_minute` column added, `date_minute` column created then disgarded")
  }
  
  return(dataset)
}
```

```{r function_qc}
apply_qc <- function(dataset, avg_ab = TRUE){
  
  # Creating columns to average A & B data
  if (avg_ab == TRUE) {
    dataset <- dataset %>% 
      rowwise() %>% 
      mutate(
        pm25_cf1 = mean(c(pm25_cf1_A, pm25_cf1_B), na.rm = TRUE),
        pm25_atm = mean(c(pm25_atm_A, pm25_atm_B), na.rm = TRUE)
      ) %>% 
      ungroup()
    print("Columns for averages of A & B data added")
  }
  
  dataset <- dataset %>% 
    # Basic quality control; only physically possible values (and real numbers) are kept
    filter_at(
      # Selecting the columns that start with 'pm25'
      vars(starts_with("pm25")),
      # Filtering said columns such that only values 0:2000 are kept
      all_vars(between(., 0, 2000))
    ) %>% 
    filter(
      # Filtering temperature for -40:185
      between(temperature, -40, 185),
      # Filtering humidity for 0:100
      between(humidity, 0, 100)
    )
  
  return(dataset)
}
```

```{r data_qc}
# Basic subsetting and quality control

data_meta <- raw_meta %>% 
  # Selecting only variables of interest, to save on storage
  select(site_id, DEVICE_LOCATIONTYPE, label, longitude, latitude, timezone, flag_highValue)

data_pm25 <- raw_data %>% 
  # Applying quality control (see above function)
  apply_qc() %>% 
  # Adjusting to be reported time zone (see above function)
  adjust_timezone() %>% 
  # Reordering variables for ease of reading
  select(site_id, datetime, everything())

if (input_drop_hi == TRUE) {
  hi_monitors <- data_meta %>% 
    filter(flag_highValue == TRUE)
  
  print("Monitors flagged as high value to be filtered out:")
  print(hi_monitors$label)
  
  data_pm25 <- data_pm25 %>% 
    filter(!site_id %in% hi_monitors$site_id)
  
  remove(hi_monitors)
}
```

```{r eclean_qc, eval=FALSE}
remove(apply_qc, adjust_timezone)
```

```{r function_epa}
apply_epa <- function(dataset, by_day = TRUE, by_hour = FALSE, epa_percent = 75, keep_cols = FALSE) {
  
  if (by_day == FALSE & by_hour == FALSE) { stop("INPUT ERROR: Please set `by_day` and/or `by_hour` to TRUE.") }
  
  if (by_day == TRUE & by_hour == FALSE) {
    print("Grouping by date (24 hour averages, by day) [default]")
    
    dataset_stamped <- dataset %>% 
      column_dt(c("date", "date_hour"))
    
    groupings_drop <- vars(site_id, date, date_hour)
    groupings <- vars(site_id, date)
    time_unit <- 24
  }
  if (by_day == TRUE & by_hour == TRUE) {
    print("Grouping by date and hour (1 hour averages, by day)")
    
    dataset_stamped <- dataset %>% 
      column_dt(c("date_hour", "hour_minute"))

    groupings_drop <- vars(site_id, date_hour, hour_minute)
    groupings <- vars(site_id, date_hour)
    time_unit <- 60/2
  }
  if (by_day == FALSE & by_hour == TRUE) {
    print("Grouping by hour (1 hour averages)")
    
    dataset_stamped <- dataset %>% 
      column_dt(c("hour", "hour_minute"))
    
    groupings_drop <- vars(site_id, hour, hour_minute)
    groupings <- vars(site_id, hour)
    
    time_unit <- 60/2
  }
  
  # Calculating the minimum number of data points required to be included in the set
  count_to_drop <- ceiling(time_unit*(epa_percent/100))
  
  # Creating a data frame of groups to be removed due to low data quantity
  drop_quantity <- dataset_stamped %>% 
    group_by_at(groupings_drop) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    group_by_at(groupings) %>% 
    count() %>% 
    filter(n < count_to_drop) %>% 
    arrange(n)
  
  print("Values to be dropped via an anti-join due to low data quantity:")
  print(drop_quantity)
  
  drop_ab <- dataset %>% 
    select(!intersect(c("temperature", "humidity", "pm25_cf1", "pm25_atm"), colnames(.))) %>% 
    column_dt("date") %>% 
    group_by(site_id, date) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    mutate(
      # Difference between A & B sensors
      diff = pm25_cf1_A-pm25_cf1_B,
      # Percentage difference between A & B sensors
      per_diff = 100*(abs(pm25_cf1_A-pm25_cf1_B))/((pm25_cf1_A+pm25_cf1_B)/2),
      drop = case_when(
        # Will drop the following based on EPA recommendations
        abs(diff) >= 5 & abs(per_diff) >= 62 ~ TRUE,
        # Fills all other values (the ones to be kept) with 'FALSE'
        TRUE ~ FALSE
        )
      ) %>% 
    filter(drop == TRUE) %>% 
    select(site_id, date)
  
  print("Days (by sensor) to be dropped via an anti-join due to A & B sensor disagreement:")
  print(drop_ab)
  
  dataset_stamped <- dataset_stamped %>% 
    anti_join(drop_quantity) %>% 
    anti_join(drop_ab) %>% 
    group_by_at(groupings) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      # US-wide correction factor published in 2020
      epa_2020 = 0.524*pm25_cf1 - 0.0852*humidity + 5.72,
      # US-wide correction factor published in late 2020
      epa_2021 = case_when(
        pm25_cf1 <= 343 ~ 0.52*pm25_cf1 - 0.086*humidity + 5.75,
        pm25_cf1 > 343 ~ 0.46*pm25_cf1 + (3.93*10^(-4))*(pm25_cf1^2) +2.97
      ),
      # US-wide correction factor published in late 2020 using CF=ATM values
      epa_atm = case_when(
        pm25_atm < 50 ~ 0.25*pm25_atm - 0.086*humidity + 5.75,
        pm25_atm >= 50 & pm25_atm < 229 ~ 0.786*pm25_atm - 0.086*humidity + 5.75,
        pm25_atm > 229 ~ 0.69*pm25_atm + (8.84*10^-4)*(pm25_atm^2) + 2.97
      )
    ) %>% 
    # Setting negative values to NA
    mutate_at(vars(epa_2020, epa_2021, epa_atm), ~replace(., which(.<0), NA))
  
  if (keep_cols == FALSE) {
    print("Dropping extraneous columns [default]")
    dataset_stamped <- dataset_stamped %>% 
      select(!intersect(c("temperature", "humidity", "pm25_cf1_A", "pm25_atm_A", "pm25_cf1_B", "pm25_atm_B", "pm25_cf1", "pm25_atm"), colnames(.)))
  } else { print("Keeping all columns") }
  
  return(dataset_stamped)
}
```

```{r function_lrapa}
apply_lrapa <- function(dataset, by_day = TRUE, by_hour = FALSE, keep_cols = FALSE) {
  
  if (by_day == FALSE & by_hour == FALSE) {
    stop("INPUT ERROR: Please set `by_day` and/or `by_hour` to TRUE.")
  }
  if (by_day == TRUE & by_hour == FALSE) {
    print("Grouping by date (24 hour averages, by day) [DEFAULT]")
    dataset <- dataset %>% column_dt("date")
    groupings <- vars(site_id, date)
  }
  if (by_day == TRUE & by_hour == TRUE) {
    print("Grouping by date and hour (1 hour averages, by day)")
    dataset <- dataset %>% column_dt("date_hour")
    groupings <- vars(site_id, date_hour)
  }
  if (by_day == FALSE & by_hour == TRUE) {
    print("Grouping by hour (1 hour averages)")
    dataset <- dataset %>% column_dt("hour")
    groupings <- vars(site_id, hour)
  }
  
  dataset <- dataset %>% 
    group_by_at(groupings) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      lrapa = case_when(pm25_cf1 <= 65 ~ 0.5 * pm25_atm - 0.66)
      ) %>% 
    # Setting negative values to NA
    mutate_at(vars(lrapa), ~replace(., which(.<0), NA))
  
  if (keep_cols == FALSE) {
    print("Dropping extraneous columns [default]")
    dataset <- dataset %>% 
      select(!intersect(c("temperature", "humidity", "pm25_cf1_A", "pm25_atm_A", "pm25_cf1_B", "pm25_atm_B", "pm25_cf1", "pm25_atm"), colnames(.)))
  } else { print("Keeping all columns") }
  
  return(dataset)
}
```

```{r function_corrections}
apply_corrections <- function(dataset, daily = TRUE, hourly = FALSE, epa = 75){
  epa <- apply_epa(dataset, by_day = daily, by_hour = hourly, epa_percent = epa, keep_cols = TRUE)
  print("EPA corrections applied")
  lrapa <- apply_lrapa(dataset, by_day = daily, by_hour = hourly, keep_cols = TRUE)
  print("LRAPA corrections applied")
  
  dataset_corrected <- full_join(epa, lrapa)
  print("Data frame of corrected values created")
  return(dataset_corrected)
}
```

```{r function_tag_dates}
apply_date_tags <- function(dataset,
                        starts = input_date_starts, ends = input_date_ends, tags = input_date_tags,
                        date_stamp = "17 Jan 1999"){
  
  starts <- as.Date(starts)
  ends <- as.Date(ends)
  tags <- factor(tags)
  date_format <- stamp_date(date_stamp)
  
  data_temp <- data.frame(starts, ends, tags) %>% 
    arrange(starts) %>% 
    mutate(
      start_stamp = date_format(starts),
      end_stamp = date_format(ends),
      date_tag = fct_inorder(paste0(tags, "\n(", start_stamp, " - ", end_stamp, ")"))
    ) %>% 
    pivot_longer(cols = c(starts, ends), values_to = "date") %>% 
    group_by(date_tag) %>% 
    complete(date = full_seq(date, 1)) %>% 
    select(!c(name, tags, start_stamp, end_stamp))
  
  remove_date <- FALSE
  
  if ("date" %in% colnames(dataset) == FALSE) {
    print("`date` column not detected; temporarily adding to dataframe")
    dataset <- column_dt(dataset, "date")
    remove_date <- TRUE
  }
  
  dataset <- dataset %>% 
    left_join(data_temp) %>% 
    drop_na(date_tag)
  
  if (remove_date == TRUE) {
    dataset <- select(dataset, !date)
    print("Temporary `date` column removed")
  }
  
  return(dataset)
}
```

```{r function_tag_hours}
apply_hour_tags <- function(dataset, starts = input_hour_starts, tags = input_hour_tags){
  
  
  if ("hour" %in% colnames(dataset) == FALSE & "date_hour" %in% colnames(dataset) == FALSE) {
    stop("Data set does not contain hours. Execution halted.")
  }
  
  if ("hour" %in% colnames(dataset) == FALSE) {
    print("`hour` column not detected; temporarily adding to dataframe")
    dataset <- column_dt(dataset, "hour")
    remove_hour <- TRUE
  } else { remove_hour <- FALSE }
  
  tags <- factor(tags)
  
  hours_in_day <- data.frame(starts = 0:23)
  
  data_temp <- data.frame(starts, tags) %>% 
    mutate(
      ends = lead(starts - 1),
      ends = replace_na(ends, starts[1]-1),
      tags = paste0(tags, "\n(", formatC(starts, width=2, flag=0), ":00-", formatC(ends, width=2, flag=0), ":59)")
    ) %>% 
    arrange(starts) %>% 
    mutate(hour_tag = fct_inorder(tags)) %>%
    complete(starts = full_seq(starts, 1)) %>%
    right_join(hours_in_day) %>%
    rename(hour = starts) %>%
    fill(hour_tag) %>%
    select(!c(tags, ends)) %>%
    mutate(hour = hms::as_hms(hour*60*60))
  
  dataset <- dataset %>% 
    left_join(data_temp)
  
  if (remove_hour == TRUE) {
    dataset <- select(dataset, !hour)
    print("Temporary `hour` column removed")
  }
  
  return(dataset)
}
```

```{r function_applying}
apply_functions <- function (dataset, by_day = TRUE, by_hour = FALSE, tag_dates = run_date_grouping, tag_hours = run_hour_grouping) {
 
  dataset <- apply_corrections(dataset, daily = by_day, hourly = by_hour)
  print("Correction factors applied.")
  
  if (by_day == TRUE & tag_dates == TRUE) {
    dataset <- apply_date_tags(dataset)
    print("Data now tagged by provided date groupings")
  } else {
    print("Date groups not applied")
  }
  
  if (by_hour == TRUE & tag_hours == TRUE) {
    dataset <- apply_hour_tags(dataset)
    print("Data now tagged by provided hour groupings")
  } else {
    print("Hour groups not applied")
  }
  
  return(dataset)
}
```

```{r data_applying}
data_hourly <- apply_functions(data_pm25, by_day = TRUE, by_hour = TRUE)
data_daily <- apply_functions(data_pm25, by_day = TRUE, by_hour = FALSE)
data_diurnal <- apply_functions(data_pm25, by_day = FALSE, by_hour = TRUE)
```

```{r eclean_applying, eval=FALSE}
remove(apply_corrections, apply_functions, apply_epa, apply_lrapa, apply_hour_tags, apply_date_tags)
remove(list = c(ls(pattern = "input_(hour|date)_")))
```

```{r viz_function_map}
# FUNCTION
# Required inputs: pm_data (df), value (column of PM2.5 data w/in df to map), location_data (lat/long data to be joined to df; default: data_meta)
# Optional inputs: grouping variables (1 or 2) w/ respective custom exclusions; filter for indoor/outdoor data
# Other customization for map: type (stamen), zoom, tint & color, point size, color scale (virids)

map_q <- function(pm_data, value, location_data = data_meta,
                  grouping_var = NULL, exclude = NULL,
                  grouping_var_2 = NULL, exclude_2 = NULL,
                  outside = include_outside, inside = include_inside,
                  label = c(""),
                  maptype = "toner-lite", zoom = 11, tint = 0.5, tint_color = "black",
                  point_size = 3, viridis = "viridis"){
  
  # Warning message and halting execution if required input is missing.
  if (missing(value) == TRUE) {
    stop ("ERROR in argument 'value': Missing input, with no default. Execution haulted. Please input a valid PM2.5 variable (e.g. epa_2021).")
  }
  
  # Warning message and halting execution if inputeded data does not have more than one unique site_id (map will bug)
  if (length(unique(pm_data$site_id)) == 1) {
    stop("ERROR: Insufficient data to create a map. Provide a data set with more than one site.")
  }
  
  input_labels <- paste0("(", paste(label, collapse = ")|("), ")")
  
  # Wrangling based on provided inputs
  data_temp <- pm_data %>% 
    ungroup() %>% 
    # Grouping by provided variables, as well as site_id
    group_by_at(vars(site_id, {{grouping_var}}, {{grouping_var_2}})) %>% 
    # Calculating means by said grouping variables
    summarize(mean = mean({{value}}, na.rm = TRUE)) %>% 
    drop_na(mean) %>% 
    # Adding location data to get lat & lon
    left_join(location_data) %>% 
    # Selecting desired labels, if provided
    filter(str_detect(label, input_labels))
  
  # Feedback message
  print("Data now grouped and averaged. Location data added.")
  
  # Custom filtering: exclusions for input categories
  if (is.null(exclude) == FALSE) {
    to_exclude <- paste(exclude, collapse = "|")
    data_temp <- data_temp %>% 
      filter(!str_detect({{grouping_var}}, to_exclude))
    print(paste("Excluded", exclude, "from", deparse(substitute(grouping_var))))
  }
  if (is.null(exclude_2) == FALSE) {
    to_exclude_2 <- paste(exclude_2, collapse = "|")
    data_temp <- data_temp %>% 
      filter(!str_detect({{grouping_var_2}}, to_exclude_2))
    print(paste("Excluded", exclude_2, "from", deparse(substitute(grouping_var_2))))
  }
  
  # Default shapes for inside/outside
  shapes_inout <- c("outside" = 21, "inside" = 23)
  
  # Custom filtering: exclusions for inside/outside
  # Will also override shape arguments to prevent excluded category from showing in the legend
  if (outside == FALSE) {
    data_temp <- data_temp %>% 
      filter(DEVICE_LOCATIONTYPE != "outside")
    print("Outdoor data excluded")
    shapes_inout <- c("inside" = 23)
  }
  if (inside == FALSE) {
    data_temp <- data_temp %>% 
      filter(DEVICE_LOCATIONTYPE != "inside")
    print("Indoor data excluded")
    shapes_inout <- c("outside" = 21)
  }
  
  
  # Labels for the plot
  lab_title <- expression("PM"[2.5]*" values")
  lab_subtitle <- paste("Data from PurpleAir. Data plotted:", deparse(substitute(value)))
  lab_pm <- expression(paste("Average PM"[2.5]*" (", mu, "g/m"^3*"):"))
  
  # Base plot
  plot <- qmplot(data = data_temp, x = longitude, y = latitude,
         geom = "blank", maptype = maptype, zoom = zoom, darken = c(tint, tint_color)) +
    facet_wrap(vars({{grouping_var}})) +
    geom_point(
      aes(fill = mean, shape = DEVICE_LOCATIONTYPE),
      color = "white",
      alpha = 0.8,
      size = point_size
    ) +
    scale_fill_viridis(
      option = {{viridis}},
      direction = -1,
      end = 0.85,
      limits = c(0, NA)
    ) +
    theme_void() +
    scale_shape_manual(values = shapes_inout) +
    theme(
      legend.position = "bottom",
      legend.box = "vertical"
    ) + 
    guides(shape = guide_legend(override.aes = list(fill="black"))) +
    labs(
      title = lab_title,
      subtitle = lab_subtitle,
      fill = lab_pm,
      shape = "Monitor location:",
      caption = "Data not grouped."
    )
  
  # Feedback message
  print("Base plot created.")
  
  ## Faceting by grouping variable(s) and applying an appropriate caption to reflect the grouping
  # Facet wrap if 1 grouping variable provided
  if (deparse(substitute(grouping_var)) != "NULL" & deparse(substitute(grouping_var_2)) == "NULL") {
    plot <- plot +
      facet_wrap(vars({{grouping_var}})) +
      labs(
        caption = paste(
          "Data grouped by",
          (colnames(pm_data %>% select({{grouping_var}})))[1]
        )
      )
    # Feedback message
    print(paste("Plot now faceted by", deparse(substitute(grouping_var))))
  }
  # Facet grid if 2 grouping variables provided
  if (deparse(substitute(grouping_var)) != "NULL" & deparse(substitute(grouping_var_2)) != "NULL") {
    plot <- plot +
      facet_grid(
      formula(paste(
        vars({{grouping_var_2}}),
        "~",
        vars({{grouping_var}})
      ))) +
      labs(
        caption = paste(
          "Data grouped by",
          deparse(substitute(grouping_var)),
          "and",
          deparse(substitute(grouping_var_2))
          )
      )
    # Feedback message
    print(paste("Plot now faceted by",
                deparse(substitute(grouping_var)),
                "and",
                deparse(substitute(grouping_var_2))
                ))
  }
  
  print("Final plot created.")
  
  # Returning the final plot
  plot
}
```

```{r test_map, eval=FALSE}
map_q(data_hourly, value = epa_2021, grouping_var = hour_tag)

data_hourly %>% 
  column_dt("hour") %>% 
  mutate(hour = as.factor(hour)) %>% 
  map_q(value = epa_2021, grouping_var = hour, grouping_var_2 = date_tag, inside=FALSE)

map_q(data_daily, value = epa_2020, grouping_var = date_tag, inside = FALSE)

map_q(data_daily, value = lrapa, grouping_var = date_tag, exclude = "Fire")

map_q(data_hourly, value = pm25_atm, grouping_var = hour_tag, exclude = "Afternoon", point_size = 5,
      maptype = "terrain", tint = 0.3, tint_color = "white", viridis = "magma")
```

```{r viz_function_single_heatmap}
heatmap_single <- function(dataset = data_hourly, variable_of_interest, site_of_interest,
                           data_labels = TRUE, digits = 2, date_breaks = "1 day",
                           location_data = data_meta){
  
  if ("date_hour" %in% colnames(dataset) == FALSE) { stop("ERROR: Inputted data set is not hourly by day") }
  
  variable_of_interest_qt <- deparse(substitute(variable_of_interest))
  
  if (variable_of_interest_qt %in% colnames(dataset) == FALSE) {
    dataset %>%
      slice(1) %>%
      ungroup() %>%
      select_if(is.numeric) %>%
      colnames() %>%
      print()
    stop("ERROR: Inputted variable of interest is not in the provided data set. Execution halted. Valid inputs are listed above")
  }
  
  temp_loc <- location_data %>% 
    select(site_id, label)
  
  temp_df <- dataset %>% 
    ungroup() %>% 
    # Adding labels to the data set
    left_join(temp_loc) %>% 
    # Filtering data set based on inputted site of interest
    filter(str_detect(label, site_of_interest)) %>% 
    # Removing empty values from variable of interest (otherwise is grey when mapping)
    drop_na({{variable_of_interest}}) %>% 
    # Removing duplicate rows
    distinct()
  
  # Verification that only one monitor is selected. Execution will halt if 0 or >1 are detected
  if (length(unique(temp_df$label)) == 0) {
    stop("ERROR: No monitors selected. Please verify that a distinct label was provided. Capitalization matters.")
  }
  if (length(unique(temp_df$label)) > 1) {
    print("Matching locations from meta data:")
    temp_loc %>% filter(str_detect(label, site_of_interest)) %>% distinct() %>% pull(label) %>% print()
    print("Matching locations that contain values of interest:")
    temp_df %>% pull(label) %>% unique() %>% print()
    stop("ERROR: More than one site selected. Please use a more precise string argument; matching values are listed above.")
  }
  
  unit_results <- settings_units(dataset = dataset, var = variable_of_interest_qt, cap_color = NA,
                                 lab_title = "Heatmap of")
  
  lab_title <- unit_results$lab_title
  lab_title_val <- unit_results$lab_title_val
  lab_subtitle <- paste0("Monitor selected: \"", temp_df %>% pull(label),
                         "\" (ID: ", temp_df %>% pull(site_id),").\n",
                        unit_results$lab_subtitle)
  lab_fill <- unit_results$lab_fill
  fill_colors <- unit_results$fill_colors
  
  scale_results <- settings_dt_scale(dataset = dataset, start_date = NA, end_date = NA)
  
  dataset <- scale_results$dataset
  lab_title_sub <- scale_results$lab_title_sub
  lab_caption <- scale_results$lab_caption
  
  if (data_labels == TRUE) {
    data_labels <- geom_text()
  } else { data_labels <- NULL }
  
  # Plotting data
  temp_df %>% 
    ggplot(aes(x = date(date_hour),
               y = hour(date_hour),
               fill = {{variable_of_interest}},
               label = rounding_w_zeroes({{variable_of_interest}}, digits)
    )) +
    geom_tile() +
    fill_colors +
    data_labels +
    # Setting scale breaks such that 0:23 will always display, regardless of data availability
    scale_y_continuous(trans = "reverse", # Reverse such that morning is at top, night at bottom
                       labels = function(x) sprintf("%02d:00", x), # labeling with proper hourly formatting
                       breaks = 0:23, # breaks every hour
                       limits = c(24,-1)) +
    scale_x_date(breaks = date_breaks,
                 expand = c(0.01, 0.01),
                 date_labels = "%d %b") +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      panel.grid = element_blank(),
      axis.title.x = element_blank(),
      axis.text.x = element_text(angle = 30, hjust = 1),
      axis.ticks.x = element_line()
    ) +
    labs(
      title = paste(lab_title, lab_title_val, lab_title_sub),
      subtitle = lab_subtitle,
      fill = lab_fill,
      caption = lab_caption,
      y = "Hour of the day"
    ) + 
    guides(fill = guide_colorbar(barwidth = 10))
}
```

```{r test_heatmap_single, eval=FALSE}
# heatmap_single(data_hourly, "Indigo", epa_2021)
# heatmap_single(data_hourly, "Indigo", humidity)
heatmap_single(data_hourly, epa_2021, "Sunset")
```

```{r viz_function_heatmap}
heatmap_cross <- function(dataset, variable_of_interest, drop_incomplete = FALSE,
                          cap_value = NA, cap_color = red,
                          location_data = data_meta, digits = 2,
                          start_date = input_startdate, end_date = input_enddate){
  
  # Dropping NA values from the variable of interest
  # Will appear as gaps if viewing the complete viz, rather than gray
  dataset <- dataset %>% 
    drop_na({{variable_of_interest}})
  
  variable_of_interest_qt <- deparse(substitute(variable_of_interest))
  
  # If monitors with missing values for any time stamp are to be dropped
  if (drop_incomplete == TRUE) {
    # Number of values expected for a complete set
    complete_num <- (dataset %>% 
                       ungroup() %>% 
                       count(site_id) %>% 
                       arrange(desc(n)) %>% 
                       pull(n))[1]
    
    # List of monitors with incomplete sets
    to_drop <- dataset %>% 
      ungroup() %>% 
      count(site_id) %>% 
      filter(n != complete_num) %>% 
      pull(site_id)
    
    # Feedback
    print("Monitors with incomplete temporal data that will be dropped:")
    print(paste(to_drop))
    
    # Removing incomplete monitors
    dataset <- dataset %>% 
      filter(!site_id %in% to_drop)
    
  } else { print("All monitors will be plotted.") }
  
  # Adding quotation marks
  cap_color <- deparse(substitute(cap_color)) %>%
    str_replace_all("\\\"", "") # Removing extra quotation marks if already provided
  
  # Location data
  temp_loc <- location_data %>% 
    # Arranging such that northern-most monitors will be on top
    mutate(label = fct_reorder(as.factor(label), desc(latitude))) %>% 
    # Selecting only variables of interest to save space
    select(site_id, label, DEVICE_LOCATIONTYPE)
  
  unit_results <- settings_units(dataset = dataset, var = variable_of_interest_qt, cap_color = cap_color,
                                 lab_title = "Heatmap of")
  
  lab_title <- unit_results$lab_title
  lab_title_val <- unit_results$lab_title_val
  lab_subtitle <- unit_results$lab_subtitle
  lab_fill <- unit_results$lab_fill
  fill_colors <- unit_results$fill_colors
  
  cap_guide <- guides(fill = guide_colorbar(order = 1, barwidth = 10),
                      color = "none")
  
  # If manually applying a max filter value
  if (is.na(cap_value) == FALSE) {
    # Getting number of rows at or above the cap
    nrow_hi <- dataset %>% 
      filter({{variable_of_interest}} >= cap_value) %>% 
      nrow()
    
    if ((nrow_hi > 0) == TRUE) {
      # Replacing the values above the set max to be NA so that they will be colored differently on the map
      dataset <- dataset %>% 
        mutate_at(vars({{variable_of_interest}}), ~replace(., which(.>={{cap_value}}), NA))
      
      # Updated lab caption to include the filter
      lab_subtitle <- paste0("Color scale manually capped at ",
                             cap_value, " units; all higher values colored ", cap_color,
                             ".\n", lab_subtitle)
      
      # Feedback
      print(paste("Values greater than or equal to",
                  {{cap_value}},
                  "in",
                  variable_of_interest_qt,
                  "will be colored",
                  cap_color))
      
      cap_guide <- guides(fill = guide_colorbar(order = 1, barwidth = 10),
                          color = guide_legend(
                            title = paste0(cap_value, "+"),
                            order = 2,
                            title.position = "bottom",
                            title.theme = element_text(size = 10),
                            override.aes = list(color = cap_color, fill = cap_color)
                          ))
    } else {
      print(paste0("No values greater than or equal to ",
                   {{cap_value}},
                   " found in ",
                   variable_of_interest_qt,
                   "; color cap will not be applied."))
    }
  }
  
  scale_results <- settings_dt_scale(dataset = dataset, start_date = start_date, end_date = end_date)
  
  dataset <- scale_results$dataset
  lab_title_sub <- scale_results$lab_title_sub
  x_angle <- scale_results$x_angle
  x_scale <- scale_results$x_scale
  lab_caption <- scale_results$lab_caption
  
  lab_caption <- paste0(lab_caption, "\nMonitors are arranged north to south in their respective groups.")
  
  
  dataset %>% 
    distinct() %>% 
    left_join(temp_loc) %>% 
    ggplot(aes(
      x = timestamp,
      y = label,
      color = "",
      fill = {{variable_of_interest}}
    )) +
    facet_grid(DEVICE_LOCATIONTYPE~., scales = "free_y", space = "free_y") +
    geom_tile() +
    fill_colors + 
    scale_y_discrete(limits = rev) + 
    theme_minimal() +
    theme(
      legend.position = "bottom",
      panel.grid = element_blank(),
      panel.border = element_blank(),
      axis.text.x = element_text(angle = x_angle, hjust = 1),
      axis.title = element_blank(),
      axis.ticks.x = element_line()
    ) + 
    labs(
      title = paste(lab_title, lab_title_val, lab_title_sub),
      subtitle = lab_subtitle,
      fill = lab_fill,
      caption = lab_caption
    ) +
    x_scale + 
    scale_color_manual(values = "transparent") +
    cap_guide
}
```

```{r test_heatmap, eval=FALSE}
#heatmap_cross(data_pm25, variable_of_interest=pm25_atm, cap_value = 50, drop_incomplete = TRUE)
# heatmap_cross(data_pm25, pm25_atm, cap_value = 50, cap_color = green)
heatmap_cross(data_hourly, pm25_atm)
heatmap_cross(data_daily, temperature)
heatmap_cross(data_diurnal, pm25_atm, cap_value = 15)
```

```{r viz_function_ts}
ts_line <- function(dataset, variable_of_interest,
                    add_points = FALSE,
                    single_column = FALSE, include_avg = TRUE,
                    label = c(""),
                    digits = 2, label_length = 10,
                    location_data = data_meta,
                    start_date = input_startdate, end_date = input_enddate){
  
  variable_of_interest_qt <- deparse(substitute(variable_of_interest))
  
  # Dropping NA values from the variable of interest
  dataset <- dataset %>% 
    mutate("var" = {{variable_of_interest}}) %>% 
    drop_na(var)
  
  location_data <- location_data %>% 
    inner_join((dataset %>% distinct(site_id))) %>% 
    mutate(label = make.unique(label, sep = " "))
  
  if (single_column == TRUE) {
    facet <- facet_grid(label~.)
    print("Charts will be arranged in a single column (multiple rows).")
    location_data <- location_data %>% mutate(label = str_wrap(label, label_length))
    print("Line breaks added to labels")
  } else {
    facet <- facet_wrap(~label)
    print("Charts will be arranged in multiple rows and columns.")
  }
  
  scale_results <- settings_dt_scale(dataset = dataset, start_date = start_date, end_date = end_date)
  
  dataset <- scale_results$dataset
  lab_title_sub <- scale_results$lab_title_sub
  x_angle <- scale_results$x_angle
  x_scale <- scale_results$x_scale
  lab_dates <- scale_results$lab_caption
  lab_caption <- paste("Black lines indiciate the data for a specific monitor of interest.",
                       "Gray lines represent data from all monitors.",
                       "\nMaximum and minimum values by monitor are marked by orange and blue text, respectively.")
  
  unit_results <- settings_units(dataset = dataset, var = variable_of_interest_qt, cap_color = NA,
                                 lab_title = "Timeseries of")
  
  lab_title <- unit_results$lab_title
  lab_title_val <- unit_results$lab_title_val
  lab_subtitle <- unit_results$lab_subtitle
  lab_fill <- unit_results$lab_fill
  y_lab <- lab_fill
  fill_colors <- unit_results$fill_colors
  
  input_labels <- paste0("(", paste(label, collapse = ")|("), ")")
  
  extrema <- dataset %>% 
    left_join({{location_data}}) %>% 
    filter(str_detect(label, input_labels)) %>%
    select(site_id, label, timestamp, var) %>% 
    group_by(site_id) %>% 
    mutate(
      max = case_when(var == max(var, na.rm = TRUE) ~ var),
      min = case_when(var == min(var, na.rm = TRUE) ~ var),
      date = date(timestamp)
    ) %>% 
    distinct() %>% 
    group_by(site_id, date) %>% 
    mutate_at(vars(max, min), funs(replace(., duplicated(.), NA))) %>% 
    select(!date)
  
  label_order <- location_data %>% 
    select(site_id, label, latitude) %>% 
    distinct() %>% 
    mutate(label = fct_reorder(as.factor(label), desc(latitude))) %>% 
    pull(label)
  
  if (length(unique(extrema$site_id)) == 1) {
    include_avg <- FALSE
    print("One monitor detected. Not plotting average.")
  }
  
  if(include_avg == TRUE){
    extrema_avg <- dataset %>% 
      ungroup() %>% 
      select(timestamp, var) %>% 
      group_by(timestamp) %>% 
      summarize(var = mean(var, na.rm = TRUE)) %>% 
      mutate(
        max = case_when(var == max(var, na.rm=TRUE) ~ var),
        min = case_when(var == min(var, na.rm=TRUE) ~ var)
      ) %>% 
      mutate(
        label = "AVERAGE",
        site_id = "Averages"
      ) %>% 
      distinct()
    
    extrema <- rbind(extrema, extrema_avg)
    
    # Appending the "average" label onto the set
    label_order <- fct_inorder(c(levels(label_order), "AVERAGE"))
    
    print("Average data added.")
    
    lab_caption <- paste(lab_caption, "\nThe green graph represents the moving average of all monitors in the data set.")
    
  } else {
    label_order <- fct_inorder(levels(label_order))
    print("No averages will be added.")
  }
  
  if (add_points == TRUE) {
    data_points <- geom_point(aes(fill = var), alpha = 0.7, shape = 21, stroke = 0)
    print("Data points will be added.")
  } else { data_points <- NULL }
  
  dataset %>% 
    select(timestamp) %>% 
    crossing(label = as.character(label_order)) %>% 
    left_join(extrema) %>%
    mutate(label = factor(label, label_order)) %>%
    drop_na(var) %>%
    mutate(
      max = case_when(!is.na(max) ~ rounding_w_zeroes(max, digits)),
      min = case_when(!is.na(min) ~ rounding_w_zeroes(min, digits))
    ) %>% 
    ggplot(aes(x = timestamp, y = var)) +
    facet +
    # geom_hline(yintercept = 0, linetype = "longdash", alpha = 0.5) +
    geom_line(
      data = dataset %>% drop_na(var),
      aes(group = site_id),
      color = "gray",
      alpha = 0.5
    ) +
    geom_line(
      data = . %>% filter(site_id!="Averages"),
      color = "black"
    ) +
    geom_line(
      data = . %>% filter(site_id=="Averages"),
      color = "darkgreen"
    ) +
    data_points + 
    fill_colors +
    geom_point(
      data = . %>% filter(!is.na(max)),
      color = "darkorange",
      shape = 1
    ) +
    geom_point(
      data = . %>% filter(!is.na(min)),
      color = "royalblue",
      shape = 1
    ) +
    geom_text_repel(
      data = . %>% mutate(max = replace_na(max, "")),
      aes(label = max),
      min.segment.length = 0,
      box.padding = 0.75,
      max.overlaps = Inf,
      color = "darkorange"
    ) +
    geom_text_repel(
      data = . %>% mutate(min = replace_na(min, "")),
      aes(label = min),
      min.segment.length = 0,
      box.padding = 0.75,
      max.overlaps = Inf,
      color = "royalblue"
    ) +
    theme_minimal() +
    x_scale +
    theme(
      panel.spacing.x = unit(5, "mm"),
      legend.position = "bottom",
      panel.grid = element_blank(),
      panel.border = element_blank(),
      axis.text.x = element_text(angle = x_angle, hjust = 1),
      axis.title.x = element_blank(),
      axis.ticks.x = element_line()
    ) +
    labs(
      title = paste(lab_title, lab_title_val, lab_title_sub),
      subtitle = paste(lab_subtitle, lab_dates, sep = "\n"),
      caption = lab_caption,
      x = "Time",
      y = y_lab,
      fill = lab_fill
    )
  
}
```

```{r test_ts, eval=FALSE}
ts_line(data_hourly, pm25_atm)
ts_line(data_hourly, pm25_atm, add_points = TRUE)
ts_line(data_hourly, pm25_atm, label = c("\\bSTAR\\b"), single_column = TRUE)
```


```{r function_dt_scale}
settings_dt_scale <- function(dataset, start_date, end_date) {
  # Expanding axis to allow monitor labels to be closer to the data
  options_expand <- expansion(mult = c(0.01, 0.01))
  x_angle <- 30
  lab_title_sub <- "across time"
  
  # Setting axis breaks based on the minimum time unit in the data set
  if ("datetime" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", unaveraged")
    dataset <- dataset %>% rename(timestamp = datetime)
    x_scale <- scale_x_datetime(breaks = "1 day", date_labels = "%d %b", expand = options_expand)
    print("Raw set detected: x-axis will map across in units of apx. 2 minutes, with axis breaks each day")
    date_in_set <- TRUE
  } else if ("date_hour" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", averaged hourly by day")
    dataset <- dataset %>% rename(timestamp = date_hour)
    x_scale <- scale_x_datetime(breaks = "1 day", date_labels = "%d %b", expand = options_expand)
    print("Hourly set detected: x-axis will map across in units of hour in each day, with axis breaks each day")
    date_in_set <- TRUE
  } else if ("date" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", averaged by day")
    dataset <- dataset %>% rename(timestamp = date)
    x_scale <- scale_x_date(breaks = "1 day", date_labels = "%d %b", expand = options_expand)
    print("Daily set detected: x-axis will map across in units of 24 hours, with axis breaks each day")
    date_in_set <- TRUE
  } else if ("hour" %in% colnames(dataset) == TRUE) {
    lab_title_sub <- paste0(lab_title_sub, ", averaged by hour of day")
    dataset <- dataset %>% mutate(time = hms::as_hms(hour), timestamp = as_datetime(time))
    x_scale <- scale_x_datetime(breaks = "1 hour", date_labels = "%H:%M", expand = options_expand)
    x_angle <- 45
    print("Diurnal set detected: Data will map across by hour of day, with axis breaks each hour")
    print("Dates in the caption will default to the inputted start and end dates")
    date_in_set <- FALSE
  } else { stop("INPUT ERROR: No time-based variables found!") }
  
  if (date_in_set == TRUE) {
    start_date <- min(date(dataset$timestamp))
    end_date <- max(date(dataset$timestamp))
  } else {
    start_date <- as.Date(start_date)
    end_date <- as.Date(end_date)
  }
  # Formatting dates
  start_date <- format(start_date, "%d %b %Y")
  end_date <- format(end_date, "%d %b %Y")
  
  lab_caption <- paste0("Data from ", start_date, " to ", end_date, ".")
  
  return(list(
    dataset = dataset,
    lab_title_sub = lab_title_sub,
    x_angle = x_angle,
    x_scale = x_scale,
    date_in_set = date_in_set,
    start_date = start_date,
    end_date = end_date,
    lab_caption = lab_caption
  ))
}
```

```{r function_units}
settings_units <- function(dataset, var, cap_color, digits, lab_title = "Graph of",
                           lab_fill = "Units", lab_unit = "units") {
  
  variable_of_interest_qt <- var
  
  # Defaults
  lab_title_val <- variable_of_interest_qt
  # y_line <- NULL
  # Color scale will default set to start at 0, regardless of minimum in data
  fill_colors <- scale_fill_viridis(option = "plasma", limits = c(0, NA), na.value = cap_color)
  
  if (str_detect(variable_of_interest_qt, "pm") == TRUE) {
    lab_title_val <- "Particulate Matter (PM)"
    # lab_unit <- expression(paste(mu, "g/m"^3))
    #lab_unit <- expression(mu*"g/m"^3)
    # lab_unit <- "g/m^3"
    lab_unit <- '*mu*"g/m"^3*'
    
    if (str_detect(variable_of_interest_qt, "(1.0)|(01)|(1$)|(1\\D)") == TRUE) {
      pm_val <- 1.0
      print("PM 1.0 detected")
    } else if (str_detect(variable_of_interest_qt, "(25)|(2.5)") == TRUE) {
      pm_val <- 2.5
      print("PM 2.5 detected")
    } else if (str_detect(variable_of_interest_qt, "(10)") == TRUE) {
      pm_val <- 10
      print("PM 10 detected")
    } else {
      pm_val <- NULL
      print("PM unit undetermined")
    }
    lab_fill <- parse(text = paste0('PM[', pm_val, ']~"("', lab_unit,'")"'))
    lab_unit <- "units"
    # y_line <- geom_hline(yintercept = 0, linetype = "longdash", alpha = 0.5)
    #bquote(PM[.(pm_val)]~"("*mu*"g/"*m^3*")")
  }
  
  # Adjusting color scale and labels if the variable of interest is internal temperature
  if (str_detect(variable_of_interest_qt, "temp") == TRUE) {
    lab_title_val <- "internal temperature"
    fill_colors <- scale_fill_viridis(option = "cividis", begin = 0.15, na.value = cap_color)
    print("Temperature detected as variable of interest; adjusting labels accordingly")
    lab_unit <- "\u00B0F"
    
    if (str_detect(variable_of_interest_qt, "_c") == TRUE) {
      lab_unit <- "\u00B0C"
      print("Temperature detected to be in Celsius")
    } else { print("Temperature assumed to be in Fahrenheit") }
    
    lab_fill <- paste0("Internal temperature (", lab_unit, ")")
  }
  
  # Adjusting color scale and labels if the variable of interest is RH
  if (str_detect(variable_of_interest_qt, "((H|h)umid)|(rh)|(RH)") == TRUE) {
    lab_title_val <- "humidity"
    lab_unit <- "%"
    lab_fill <- paste0("Relative humidity (", lab_unit, ")")
    # y_line <- geom_hline(yintercept = c(0,100), linetype = "longdash", alpha = 0.5)
    fill_colors <- scale_fill_viridis(option = "mako", direction = -1, limits = c(0, 100), end = 0.9, na.value = cap_color)
    print("RH detected as variable of interest; adjusting labels accordingly")
  }
  
  # Getting min and max values from the data set
  val_min <- dataset %>% select(variable_of_interest_qt) %>% min(na.rm = TRUE)
  val_max <- dataset %>% select(variable_of_interest_qt) %>% max(na.rm = TRUE)
  
  lab_subtitle <- paste0("Variable plotted: ",
                         variable_of_interest_qt,
                         ", with a reported range of ",
                         round(val_min, digits = digits),
                         " to ",
                         round(val_max, digits = digits),
                         " ", lab_unit, ".")
  
  return(list(
    # y_line = y_line,
    lab_title = lab_title,
    lab_title_val = lab_title_val,
    lab_subtitle = lab_subtitle,
    lab_fill = lab_fill,
    fill_colors = fill_colors
  ))
}
```

```{r function_filtering}
filter_df <- function(dataframe, location_data = data_meta, include = c(""), exclude = c("")) {
  
  input_labels <- paste0("(", paste(include, collapse = ")|("), ")")
  exclude_labels <- paste0("(", paste(exclude, collapse = ")|("), ")")
  
  data_meta <- data_meta %>% 
    filter(str_detect(label, input_labels))
  
  if(exclude_labels != "()"){
    data_meta <- data_meta %>% 
      filter(!str_detect(label, exclude_labels))
  }
  dataframe %>% 
   filter(site_id %in% (data_meta %>% pull(site_id)))
}
```

```{r,eval=FALSE}
drop_incomplete <- function(dataset) {
  # Number of values expected for a complete set
  complete_num <- (dataset %>% 
                     ungroup() %>% 
                     count(site_id) %>% 
                     arrange(desc(n)) %>% 
                     pull(n))[1]
  
  # List of monitors with incomplete sets
  to_drop <- dataset %>% 
    ungroup() %>% 
    count(site_id) %>% 
    filter(n != complete_num) %>% 
    pull(site_id)
  
  if (to_drop == character(0))
  
  # Feedback
  print("Monitors with incomplete temporal data that will be dropped:")
  print(paste(to_drop))
  
  # Removing incomplete monitors
  dataset <- dataset %>% 
    filter(!site_id %in% to_drop)
  
  return(dataset)
}
```

```{r function_rounding}
rounding_w_zeroes <- function(num, input_digits) {
  input_digits <- {{input_digits}}
  
  as.character(
    sprintf(
      paste0("%.", deparse(substitute(input_digits)), "f"),
      round(num, digits = input_digits)
    )
  )
}
```

