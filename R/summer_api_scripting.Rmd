---
title: "summer_api_scripting"
author: "Gillian McGinnis"
date: "6/23/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(AirSensor)
library(lubridate)
library(ggmap)
library(viridis)
```

```{r fixing_qmplot, eval = FALSE}
detach("package:ggmap")
remove.packages("ggmap")

devtools::install_github("dkahle/ggmap")

.rs.restartR()
library(ggmap)
```


```{r inputs}
# REQUIRED: Start and end dates of interest. Please use the following format: "YYYY-MM-DD", with leading zeros where appropriate
input_startdate <- "2020-09-01"
input_enddate <- "2020-09-30"

## REQUIRED: Inside/Outside argument. At least one of the following must be TRUE
# Grab data for outdoor sensors?
include_outside <- TRUE
# Grab data for indoor sensors?
include_inside <- TRUE

## INPUTS FOR GETTING PAS & PAT
## The following are optional but recommended. Set to NULL if not interested in filtering by the following.
# State of monitors to include. If NULL/unspecified, all monitors specified in the above boundaries will be included regardless of state
input_stateCode <- "OR"

# Boundaries (in degrees) of monitors to include. It can also evaluate n/s or e/w pairs of bounds individually.
input_north <- 45.6
input_south <- 45.4
input_east <- -122.58
input_west <- -122.854

# Labels of monitors to include. If NULL/unspecified, all monitors will be included.
# The labels are based on string detection, so for instance having "STAR" will pull all that have the word "STAR" in the label.
# Use the following format (capitalization matters):
#input_labels <- c("STAR", "Benson")
input_labels <- c("SE", "se", "Se")
#input_labels <- NULL

## DATE GROUPING: group values into tags (will not impact raw data output, but can be useful when graphing)
# Run date grouping?
run_date_grouping <- TRUE
# Date grouping categories:
input_date_tags <- c("Before", "Fire", "After")
# Start dates (in a list, "YYYY-MM-DD")
input_date_starts <- c("2020-09-01", "2020-09-10", "2020-09-20")
# End dates (in a list, "YYYY-MM-DD")
input_date_ends <- c("2020-09-09", "2020-09-19", "2020-09-30")

## HOUR GROUPING: group values into tags (will not impact raw data output, but can be useful when graphing)
# Run hour grouping?
run_hour_grouping <- TRUE
# Hour grouping categories:
input_hour_tags <- c("Morning", "Afternoon", "Evening", "Night")
# Start hours (in a list, using full hours in 24 hour format)
input_hour_starts <- c(5, 12, 17, 21)
# End hours will be automatically generated
```


```{r getting_pas}
get_area_pas <- function(state_code = input_stateCode,
                         west = input_west, east = input_east, south = input_south, north = input_north,
                         labels = input_labels,
                         datestamp = input_enddate){
  
  setArchiveBaseUrl("http://data.mazamascience.com/PurpleAir/v1")
  
  labels_string <- paste0("(",paste(labels, collapse = ")|("), ")")
  
  pas <- pas_load(
    # archival allows for retrieval of sensor info even if they are no longer reporting
    archival = TRUE,
    # datestamp is important when loading historical data, as not all monitors might be actively reporting anymore
    datestamp = str_replace_all(as.character(datestamp), "-", "")
  )
  
  if(is.null(state_code) == TRUE){
    pas_state <- pas
    print("Sensors not filtered for a specified state code.")
  } else {
    pas_state <- pas_filter(pas, stateCode == state_code)
    print(paste("Sensors now filtered for the state of", state_code))
  }
  
  pas_area <- pas_filterArea(pas = pas_state, w = west, e = east, s = south, n = north) %>% 
    pas_filter(str_detect(label, labels_string))
  
  return(pas_area)
}

pas_area <- get_area_pas()

# Cleaning environment
remove(get_area_pas, input_stateCode, input_north, input_east, input_south, input_west, input_labels)
```

```{r getting_id}
ids_outside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = TRUE)
ids_inside <- pas_getDeviceDeploymentIDs(pas_area, isOutside = FALSE)

if(include_outside == TRUE & include_inside == TRUE){
  ids <- c(ids_outside, ids_inside)
  inout_report <- ("Both indoor and outdoor sensor data will be loaded.")
}
if(include_outside == TRUE & include_inside == FALSE){
  ids <- ids_outside
  inout_report <- ("Only outdoor sensor data will be loaded.")
}
if(include_outside == FALSE & include_inside == TRUE){
  ids <- ids_inside
  inout_report <- ("Only indoor sensor data will be loaded.")
}
if(include_outside == FALSE & include_inside == FALSE){
  inout_report <- ("INPUT ERROR: Please set `include_outside` and/or `include_inside` to TRUE.")
  stop(inout_report)
}

print(inout_report)

if(length(ids) > 100){
  print("CAUTION: More than 100 monitors selected. Data processing will take time. Consider applying more area filters first.")
}
print(paste("Number of selected monitors:", length(ids)))

remove(ids_outside, ids_inside, inout_report)
```

```{r iteration}
# Setup for iteration
patList <- list()
idCount <- length(ids)
count <- 0 
successCount <- 0

# Iteration
for (id in ids[1:idCount]) {
  
  count <- count + 1
  print(sprintf("Working on %s (%d/%d) ...", id, count, idCount))
  
  # Use a try-block in case we get "no data" errors
  result <- try({
    
    patList[[id]] <- pat_createNew(
      id = id,
      label = NULL,   # Label not needed since we have the IDs
      pas = pas_area,
      startdate = input_startdate,
      enddate = input_enddate,
      baseUrl = "https://api.thingspeak.com/channels/",
      verbose = FALSE
    )
    successCount <- successCount + 1
    
  }, silent = FALSE)
  
  if ( "try-error" %in% class(result) ) {
    print(geterrmessage())
  }
  
}

# Review
print(sprintf("Successfully created %d/%d pat objects.", successCount, idCount))
```

```{r cleaning_environment}
remove(input_startdate, input_enddate, idCount, count, successCount, ids, id, result)
```


```{r transpose_data}
# patList_transposed <- transpose(patList)

# raw_meta <- bind_rows(patList_transposed[[1]], .id = "site_id")
# raw_data <- bind_rows(patList_transposed[[2]], .id = "site_id")
patList_transposed <- pmap(patList, bind_rows, .id = "site_id")
raw_meta <- patList_transposed[[1]]
raw_data <- patList_transposed[[2]]

# Cleaning environment
remove(patList_transposed)
```

```{r timezone_function}
# Time zones are reported as a variable when downloading the data. Time stamps are reported in UTC. The following will convert the time stamps to said reported time zone.
# If more than one time zone is reported in the data, the conversion will use the time zone most frequently used in the data set. This is because no more than one time zone can be applied to a date time variable.
adjust_timezone <- function(dataset, location_data = raw_meta){
  
  # Translation: If more than 1 unique timezone is reported in the data frame, then:
  if(length(unique(location_data$timezone)) > 1){
  print("Multiple time zones reported. Timestamp will be based on the most frequent time zone reported.")
  timezone <- (location_data %>% 
                 group_by(timezone) %>% 
                 count() %>% 
                 arrange(desc(n)) %>% 
                 pull(timezone))[1]
  } else {
    timezone <- unique(location_data$timezone)
  }
  
  dataset_new <- dataset %>%
    # Original time stamps (in UTC) will be preserved
    rename(datetime_utc = datetime) %>% 
    mutate(datetime = with_tz(datetime_utc, tzone = timezone))
  
  print(paste("Time zone applied:", timezone))
  return(dataset_new)
}
```

```{r qc_function}
apply_qc <- function(dataset){
  dataset %>% 
    # Basic quality control; only possible values are kept
    # This will also drop missing values
    filter_at(
      # All columns that start with 'pm25'
      vars(starts_with("pm25")),
      # Filtering such that only values 0:2000 are kept
      any_vars(between(., 0, 2000))
    ) %>% 
    filter(
      # Filtering temperature for -40:185
      between(temperature, -40, 185),
      # Filtering humidity for 0:100
      between(humidity, 0, 100)
    ) %>% 
    # Creating variables for date and hour
    mutate(
      date = date(datetime),
      date_hour = floor_date(datetime, unit = "hour")
    )
}
```

```{r qc_sets}
# Basic subsetting and quality control

data_meta <- raw_meta %>% 
  # Selecting only variables of interest, to save on storage
  select(site_id, DEVICE_LOCATIONTYPE, label, longitude, latitude, timezone)

data_pm25 <- raw_data %>% 
  # Selecting only variables of interest, to save on storage
  select(site_id, datetime, temperature, humidity, pm25_A, pm25_B, pm25_atm_A, pm25_atm_B) %>% 
  # Applying quality control (see above function)
  apply_qc() %>% 
  # Adjusting to be reported time zone (see above function)
  adjust_timezone() %>% 
  # Creating variables for date and hour
  mutate(
    date = date(datetime),
    date_hour = floor_date(datetime, unit = "hour")
  ) %>% 
  # Reordering variables for ease of reading
  select(site_id, datetime_utc, datetime, date, date_hour, everything())

# Cleaning environment
remove(adjust_timezone)
```

```{r corr_epa_factor}
apply_epa_corrections <- function(dataset, groupings){
  dataset %>% 
    group_by_at({{groupings}}) %>% 
    mutate(
      # Difference between A&B sensors
      diff = pm25_A-pm25_B,
      # Percentage difference between A&B sensors
      per_diff = 100*(abs(pm25_A-pm25_B))/((pm25_A+pm25_B)/2),
      drop = case_when(
        # We will drop the following based on EPA recommendations
        abs(diff) >= 5 & abs(per_diff) >= 62 ~ TRUE,
        # Fills all other values (the ones we'll keep) with false
        TRUE ~ FALSE
        )
      ) %>% 
    # Filtering out values which we have determined to drop
    filter(drop != TRUE) %>% 
    select(!c(drop, diff, per_diff)) %>% 
    group_by_at({{groupings}}) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      # Averaging the A and B monitors
      pm25_cf1 = mean(c(pm25_A, pm25_B), na.rm = TRUE),
      # US-wide correction factor published in 2020
      epa_2020 = 0.524*pm25_cf1 - 0.0852*humidity + 5.72,
      # US-wide correction factor published in late 2020
      epa_2021 = case_when(
        pm25_cf1 <= 343 ~ 0.52*pm25_cf1 - 0.086*humidity + 5.75,
        pm25_cf1 > 343 ~ 0.46*pm25_cf1 + (3.93*10^(-4))*(pm25_cf1^2) +2.97
        )
      )
}

data_hourly_epa <- data_pm25 %>% 
  apply_epa_corrections(groupings = vars(site_id, date, date_hour))

# Filter all dates in which there are fewer than 18/24 hr worth of measurements
data_epa_drop <- data_pm25 %>% 
  group_by(site_id, date, date_hour) %>% 
  summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
  group_by(site_id, date) %>% 
  count() %>% 
  filter(n < 18)

data_daily_epa <- data_pm25 %>% 
  anti_join(data_epa_drop) %>% 
  apply_epa_corrections(groupings = vars(site_id, date))

# Cleaning environment
remove(apply_epa_corrections, data_epa_drop)
```

```{r lrapa}
apply_lrapa_correction <- function(dataset, groupings){
  dataset %>% 
    group_by_at({{groupings}}) %>% 
    summarize_if(is.numeric, mean, na.rm = TRUE) %>% 
    rowwise() %>% 
    mutate(
      pm25_cf1 = mean(c(pm25_A, pm25_B), na.rm = TRUE),
      pm25_atm = mean(c(pm25_atm_A, pm25_atm_B), na.rm = TRUE),
      lrapa = case_when(pm25_cf1 <= 65 ~ 0.5 * pm25_atm - 0.66)
      ) %>% 
    drop_na(lrapa)
}

data_hourly_lrapa <- data_pm25 %>% 
  apply_lrapa_correction(groupings = vars(site_id, date, date_hour))

data_daily_lrapa <- data_pm25 %>% 
  apply_lrapa_correction(groupings = vars(site_id, date))

# Cleaning environment
remove(apply_lrapa_correction)
```

```{r group_sets}
data_daily <- data_daily_epa %>% 
  full_join(data_daily_lrapa)

data_hourly <- data_hourly_epa %>% 
  full_join(data_hourly_lrapa)

# Cleaning environment
remove(data_daily_lrapa, data_daily_epa, data_hourly_lrapa, data_hourly_epa)
```

```{r apply_date_grouping_set}
apply_date_grouping <- function(dataset,
                        starts = input_date_starts, ends = input_date_ends, tags = input_date_tags,
                        date_stamp = "17 Jan 1999"){
  
  starts <- as.Date(starts)
  ends <- as.Date(ends)
  tags <- factor(tags)
  date_format <- stamp_date(date_stamp)
  
  data_temp <- data.frame(starts, ends, tags) %>% 
    arrange(starts) %>% 
    mutate(
      start_stamp = date_format(starts),
      end_stamp = date_format(ends),
      date_tag = fct_inorder(paste0(tags, " (", start_stamp, " through ", end_stamp, ")"))
    ) %>% 
    pivot_longer(cols = c(starts, ends), values_to = "date") %>% 
    group_by(date_tag) %>% 
    complete(date = full_seq(date, 1)) %>% 
    select(!c(name, tags, start_stamp, end_stamp))
  
  dataset %>% 
    left_join(data_temp) %>% 
    drop_na(date_tag)
}

if(run_date_grouping == TRUE){
  data_hourly <- apply_date_grouping(data_hourly)
  data_daily <- apply_date_grouping(data_daily)
  print("Data now grouped by provided date groupings.")
  remove(input_date_starts, input_date_ends, input_date_tags)
} else {
  print("Date groups not applied.")
}

# Cleaning environment
remove(run_date_grouping)
```

```{r apply_hour_tags}
apply_hour_grouping <- function(dataset, starts = input_hour_starts, tags = input_hour_tags){
  tags <- factor(tags)
  
  hours_in_day <- data.frame(starts = 0:23)
  
  data_temp <- data.frame(starts, tags) %>% 
    mutate(
      ends = lead(starts - 1),
      ends = replace_na(ends, starts[1]-1),
      tags = paste0(tags, " (", formatC(starts, width=2, flag=0), ":00-", formatC(ends, width=2, flag=0), ":59)")
    ) %>% 
    arrange(starts) %>% 
    mutate(hour_tag = fct_inorder(tags)) %>%
    complete(starts = full_seq(starts, 1)) %>%
    right_join(hours_in_day) %>%
    rename(hour = starts) %>%
    fill(hour_tag) %>%
    select(!c(tags, ends))
  
  dataset %>% 
    mutate(hour = hour(date_hour)) %>% 
    left_join(data_temp)
}

if(run_hour_grouping == TRUE){
  data_hourly <- apply_hour_grouping(data_hourly)
  print("Data now grouped by provided hour groupings.")
  remove(input_hour_starts, input_hour_tags)
} else {
  print("Hour groups not applied.")
}

# Cleaning environment
remove(run_hour_grouping)
```


```{r map_function}
# FUNCTION
# Required inputs: pm_data (df), value (column of PM2.5 data w/in df to map), location_data (lat/long data to be joined to df; default: data_meta)
# Optional inputs: grouping variables (1 or 2) w/ respective custom exclusions; filter for indoor/outdoor data
# Other customization for map: type (stamen), zoom, tint & color, point size, color scale (virids)

map_q <- function(pm_data, value, location_data = data_meta,
                  grouping_var = NULL, exclude = NULL,
                  grouping_var_2 = NULL, exclude_2 = NULL,
                  outside = include_outside, inside = include_inside,
                  label = c(""),
                  maptype = "toner-lite", zoom = 11, tint = 0.5, tint_color = "black",
                  point_size = 3, viridis = "viridis"){
  
  # Warning message and halting execution if required input is missing.
  if(missing(value) == TRUE) stop ("ERROR in argument 'value': Missing input, with no default. Execution haulted. Please input a valid PM2.5 variable (e.g. epa_2021).")
  
  input_labels <- paste0("(", paste(label, collapse = ")|("), ")")
  
  # Wrangling based on provided inputs
  data_temp <- pm_data %>% 
    ungroup() %>% 
    group_by_at(vars(site_id, {{grouping_var}}, {{grouping_var_2}})) %>% 
    summarize(mean = mean({{value}}, na.rm = TRUE)) %>% 
    drop_na(mean) %>% 
    left_join(location_data) %>% 
    filter(str_detect(label, input_labels))
  
  print("Data now grouped and averaged. Location data added.")
  
  # Custom filtering: exclusions for input categories
  if(is.null(exclude) == FALSE){
    to_exclude <- paste(exclude, collapse = "|")
    data_temp <- data_temp %>% 
      filter(!str_detect({{grouping_var}}, to_exclude))
    print(paste("Excluded", exclude, "from", deparse(substitute(grouping_var))))
  }
  if(is.null(exclude_2) == FALSE){
    to_exclude_2 <- paste(exclude_2, collapse = "|")
    data_temp <- data_temp %>% 
      filter(!str_detect({{grouping_var_2}}, to_exclude_2))
    print(paste("Excluded", exclude_2, "from", deparse(substitute(grouping_var_2))))
  }
  
  # Default shapes for inside/outside
  shapes_inout <- c("outside" = 21, "inside" = 23)
  
  # Custom filtering: exclusions for inside/outside
  # Will also override shape arguments to prevent excluded category from showing in the legend
  if(outside == FALSE){
    data_temp <- data_temp %>% 
      filter(DEVICE_LOCATIONTYPE != "outside")
    print("Excluded outdoor data")
    shapes_inout <- c("inside" = 23)
  }
  if(inside == FALSE){
    data_temp <- data_temp %>% 
      filter(DEVICE_LOCATIONTYPE != "inside")
    print("Excluded indoor data")
    shapes_inout <- c("outside" = 21)
  }
  
  
  # Labels for the plot
  lab_title <- expression("PM"[2.5]*" values")
  lab_subtitle <- paste("Data from PurpleAir. Data plotted:", deparse(substitute(value)))
  lab_pm <- expression(paste("Average PM"[2.5]*" (", mu, "g/m"^3*"):"))
  
  # Base plot
  plot <- qmplot(data = data_temp, x = longitude, y = latitude,
         geom = "blank", maptype = maptype, zoom = zoom, darken = c(tint, tint_color)) +
    facet_wrap(vars({{grouping_var}})) +
    geom_point(
      aes(fill = mean, shape = DEVICE_LOCATIONTYPE),
      color = "white",
      alpha = 0.8,
      size = point_size
    ) +
    scale_fill_viridis(
      option = {{viridis}},
      direction = -1,
      end = 0.85,
      limits = c(0, NA)
    ) +
    theme_void() +
    scale_shape_manual(values = shapes_inout) +
    theme(
      legend.position = "bottom",
      legend.box = "vertical"
    ) + 
    guides(shape = guide_legend(override.aes = list(fill="black"))) +
    labs(
      title = lab_title,
      subtitle = lab_subtitle,
      fill = lab_pm,
      shape = "Monitor location:",
      caption = "Data not grouped."
    )
  
  print("Base plot created.")
  
  ## Faceting by grouping variable(s) and applying an appropriate caption to reflect the grouping
  # Facet wrap if 1 grouping variable provided
  if(deparse(substitute(grouping_var)) != "NULL" &
     deparse(substitute(grouping_var_2)) == "NULL"
  ){
    plot <- plot +
      facet_wrap(vars({{grouping_var}})) +
      labs(
        caption = paste(
          "Data grouped by",
          (colnames(pm_data %>% select({{grouping_var}})))[1]
        )
      )
    print(paste("Plot now faceted by", deparse(substitute(grouping_var))))
  }
  # Facet grid if 2 grouping variables provided
  if(deparse(substitute(grouping_var)) != "NULL" &
     deparse(substitute(grouping_var_2)) != "NULL"
  ){
    plot <- plot +
      facet_grid(
      formula(paste(
        vars({{grouping_var_2}}),
        "~",
        vars({{grouping_var}})
      ))) +
      labs(
        caption = paste(
          "Data grouped by",
          deparse(substitute(grouping_var)),
          "and",
          deparse(substitute(grouping_var_2))
          )
      )
    print(paste("Plot now faceted by",
                deparse(substitute(grouping_var)),
                "and",
                deparse(substitute(grouping_var_2))
                ))
  }
  
  print("Final plot created.")
  
  # Returning the final plot
  plot
}
```

```{r map_tests, eval = FALSE}

map_q(data_hourly, value = epa_2021, grouping_var = hour_tag)

map_q(data_daily, value = epa_2021, grouping_var = date_tag, label = c("STAR", "PSU"))

map_q(data_daily, value = epa_2020, grouping_var = date_tag, inside = FALSE)

map_q(data_daily, value = lrapa, grouping_var = date_tag, exclude = "Fire")

map_q(data_hourly, value = pm25_atm, grouping_var = hour_tag, exclude = "Afternoon", point_size = 5,
      maptype = "terrain", tint = 0.3, tint_color = "white", viridis = "magma")
```
